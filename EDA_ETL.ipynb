{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and ETL Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "from fetch_data import construct_urls, fetch_data_from_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Helper Functions for loading/requesting and processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(option='from_saved', start=date.today()-timedelta(days=1), end=date.today()):\n",
    "    \"\"\"\n",
    "    Function to retrieve new data from the website for specified dates, or else skip to next step.\n",
    "    \"\"\"\n",
    "    if option == 'request':\n",
    "        northbound = [[66, 82, 86, 88, 94], [132, 150, 160, 162, 164, 166], [168, 170, 172, 174]]\n",
    "        southbound = [[67, 83, 93, 95, 99], [135, 137, 139, 161, 163, 165], [167, 171, 173, 175, 195]]\n",
    "        urls = construct_urls(northbound, southbound, start, end)\n",
    "        data = fetch_data_from_urls(urls)\n",
    "    elif option == 'from_saved':\n",
    "        data = None\n",
    "        print(\"Skip this section and go to part B!\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an option\n",
    "* If `option = 'from_saved'`, go to section on Raw Data.\n",
    "* Otherwise, uncomment the other line and wait for request to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = get_data(option='from_saved')\n",
    "#start = date(2020,11,29)\n",
    "#end = date(2021,2,18)\n",
    "#data = get_data(option='request', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(num):\n",
    "    \"\"\"\n",
    "    Return direction of the train (odd = Southbound, even = Northbound).\n",
    "    \"\"\"\n",
    "    if num % 2 == 0:\n",
    "        return 'Northbound'\n",
    "    else:\n",
    "        return 'Southbound'\n",
    "\n",
    "\n",
    "def get_num(re_match):\n",
    "    \"\"\"\n",
    "    Assuming input contains a match , extract and return the numerical data from input.\n",
    "    \"\"\"\n",
    "    num_match = re.search('(?P<num>[0-9]+)', re_match)\n",
    "    return int(num_match.group('num'))\n",
    "\n",
    "\n",
    "def make_dict_from_cols(col_names):\n",
    "    \"\"\"\n",
    "    Create dictionary from a list of column names\n",
    "    \"\"\"\n",
    "    dictionary = { col_name: [] for col_name in col_names }\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_html_col_names(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Using NYP (station with both arrival times and departure times), \n",
    "    retrieve column names from the HTML table, located in the 2nd row.\n",
    "    \"\"\"\n",
    "    data_list = raw_data[arrive_or_depart]['NYP']\n",
    "    page_content = data_list[0]\n",
    "    doc = lh.fromstring(page_content)\n",
    "    tr_elements = doc.xpath('//tr')\n",
    "    html_col_names = [entry.text_content().strip() for entry in tr_elements[1]]        \n",
    "    return html_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_to_raw_df(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Function to put the raw html data in a dataframe for ease of processing.\n",
    "    \"\"\"\n",
    "    col_names = get_html_col_names(raw_data, arrive_or_depart)\n",
    "    N = 7\n",
    "    data_dict = make_dict_from_cols(['Direction', 'Station'] + col_names)\n",
    "    for station in raw_data[arrive_or_depart].keys():\n",
    "        data_list = raw_data[arrive_or_depart][station]\n",
    "        L = len(data_list)\n",
    "        for i in range(L):\n",
    "            page_content = data_list[i]\n",
    "            doc = lh.fromstring(page_content)\n",
    "            tr_elements = doc.xpath('//tr')\n",
    "            if len(tr_elements) > 3:\n",
    "                title = tr_elements[0].text_content()\n",
    "                direction = get_direction(get_num(title))\n",
    "                for j in range(2, len(tr_elements)):\n",
    "                    table_row = tr_elements[j] \n",
    "                    if len(table_row) == N:\n",
    "                        data_dict['Direction'].append(direction)\n",
    "                        data_dict['Station'].append(station)\n",
    "                        for col_name, entry in zip(col_names, table_row):\n",
    "                            data = entry.text_content()\n",
    "                            data_dict[col_name].append(data)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "            else:\n",
    "                print(\"Potentially no data for this time period, or an error occurred\", station, arrive_or_depart)\n",
    "    return pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "depart =  raw_data_to_raw_df(data, 'Depart')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive = data['Arrive']\n",
    "start_time = time.time()\n",
    "arrive = raw_data_to_raw_df(data, 'Arrive')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2020,11,29)\n",
    "end = date(2021,2,18)\n",
    "arrive_filestring = './data/trains/raw_arrive_' + str(start) + '_' + str(end) + '.csv'\n",
    "depart_filestring = './data/trains/raw_depart_' + str(start) + '_' +  str(end) + '.csv'\n",
    "print(arrive_filestring)\n",
    "print(depart_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Train Data - Scraped and Loaded into Pandas DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scraped from an HTML table, so the raw data doesn't look nice after scraping until it's put back in a dataframe. The data was then processed into an initial dataframe and saved as a CSV for later processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = pd.read_csv(arrive_filestring, lineterminator='\\n', keep_default_na=False)\n",
    "depart = pd.read_csv(depart_filestring, lineterminator='\\n', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return ['Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Ar Date', 'Sch Ar Date', \n",
    "                 'Sch Ar Day', 'Sch Ar Time','Act Ar Time', 'Arrive Diff', 'Service Disruption', 'Cancellations']\n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return [ 'Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Dp Date','Sch Dp Date', \n",
    "                 'Sch Dp Day', 'Sch Dp Time','Act Dp Time', 'Depart Diff', 'Service Disruption', 'Cancellations']\n",
    "\n",
    "    \n",
    "def get_key_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return {'Sch Full Date': 'Full Sch Ar Date', 'Sch Abbr': 'Sch Ar', 'Act Abbr': 'Act Ar', 'Diff': 'Arrive Diff'}\n",
    "    \n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return {'Sch Full Date': 'Full Sch Dp Date', 'Sch Abbr': 'Sch Dp', 'Act Abbr': 'Act Dp', 'Diff': 'Depart Diff'}\n",
    "\n",
    "\n",
    "def process_columns(df, arrive_or_depart):\n",
    "    ad_keys = get_key_names(arrive_or_depart) # the specific keys depending on if new_df is for arr or dep data\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Train Num'] = pd.to_numeric(df['Train #'])\n",
    "    new_df['Station'] = df['Station']\n",
    "    new_df['Direction'] = df['Direction']\n",
    "    \n",
    "    origin_date = pd.to_datetime(df['Origin Date'], format=\"%m/%d/%Y\", exact=False, errors='coerce')    \n",
    "    new_df['Origin Date'] = origin_date\n",
    "    new_df['Origin Year'] = origin_date.dt.year\n",
    "    new_df['Origin Quarter'] = origin_date.dt.quarter\n",
    "    new_df['Origin Month'] = origin_date.dt.month\n",
    "    new_df['Origin Day'] = origin_date.dt.day\n",
    "    new_df['Origin Week Day'] = origin_date.dt.day_name()\n",
    "    \n",
    "    sched_full_date = pd.to_datetime(df[ad_keys['Sch Abbr']], format='%m/%d/%Y %I:%M %p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Sch Full Date']] = sched_full_date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Date'] = sched_full_date.dt.date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Day'] = sched_full_date.dt.day_name()\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Time'] = sched_full_date.dt.time\n",
    "    act_time = pd.to_datetime(df[ad_keys['Act Abbr']], format='%I:%M%p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Act Abbr'] + ' Time'] = act_time.dt.time\n",
    "    \n",
    "    df['Sched Date'] = sched_full_date \n",
    "    df['Act Date'] = pd.to_datetime(sched_full_date.dt.date.astype(str) + \" \" + df[ad_keys['Act Abbr']].astype(str),exact=False, errors='coerce')\n",
    "    max_expected_delay = pd.Timedelta(hours=10)\n",
    "    delta = df['Act Date'] - df['Sched Date']\n",
    "    m_late = (delta < max_expected_delay) & (-1*max_expected_delay > delta)\n",
    "    m_early = (-1*delta < max_expected_delay) & (-1*max_expected_delay > -1*delta)\n",
    "    df.loc[m_late, 'Act Date'] += pd.Timedelta(days=1)\n",
    "    df.loc[m_early, 'Act Date'] -= pd.Timedelta(days=1)\n",
    "    new_df[ad_keys['Diff']] = np.rint((df['Act Date'] - df['Sched Date']).dt.total_seconds()/60).astype(int)\n",
    "    new_df['Service Disruption'] = df['Service Disruption'].replace('SD', 1).replace('', 0)\n",
    "    new_df['Cancellations'] =  df['Cancellations'].replace('C', 1).replace('', 0)\n",
    "    return new_df.replace('', np.nan).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_arrive.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the number of rows in each df that are omitted in the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_arrive.loc[(full_arrive['Origin Year'] == 2010)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_depart.loc[(full_depart['Origin Year'] == 2010)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files by year to break down data into smaller chunks\n",
    "* Ignore any data from 2010, this is only 23 rows in the departure and arrival dataframes combined (due to trains that were retrieved with the web request starting 1/1/2011 but originated in 2010). \n",
    "* Subset into files by arrival and departure by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "#for year in years:\n",
    "#    depart_subset = full_depart.loc[(full_depart['Origin Year'] == year)]\n",
    "#    arrive_subset = full_arrive.loc[(full_arrive['Origin Year'] == year)]\n",
    "#    print(depart_subset.shape[0], arrive_subset.shape[0])\n",
    "#    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "#    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "#    depart_subset.to_csv(depart_filestring, line_terminator='\\n', index=False)\n",
    "#    arrive_subset.to_csv(arrive_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_arrive_2021 = pd.read_csv('./data/trains/processed_arrive_2021.csv')\n",
    "#prev_depart_2021 = pd.read_csv('./data/trains/processed_depart_2021.csv')\n",
    "\n",
    "#new_arrive_2021 = pd.concat([prev_arrive_2021, full_arrive], ignore_index=True, axis=0)\n",
    "#new_depart_2021 = pd.concat([prev_depart_2021, full_depart], ignore_index=True, axis=0)\n",
    "\n",
    "#new_arrive_2021.to_csv('./data/trains/processed_arrive_2021.csv', line_terminator='\\n', index=False)\n",
    "#new_depart_2021.to_csv('./data/trains/processed_depart_2021.csv', line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Postgres Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "\n",
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS train_info CASCADE;\n",
    "        CREATE TABLE train_info (\n",
    "            train_info_id SERIAL PRIMARY KEY,\n",
    "            train_num int UNIQUE,\n",
    "            operating_direction text,\n",
    "            reg_operates_on_mon boolean,\n",
    "            reg_operates_on_tues boolean,\n",
    "            reg_operates_on_wed boolean,\n",
    "            reg_operates_on_thurs boolean,\n",
    "            reg_operates_on_fri boolean,\n",
    "            reg_operates_on_sat boolean,\n",
    "            reg_operates_on_sun boolean,\n",
    "            depart_origin_time text,\n",
    "            depart_NY_time text,\n",
    "            arrive_dest_time text\n",
    "            \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS arrivals CASCADE;\n",
    "        CREATE TABLE arrivals (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num int REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_arr_datetime timestamp,\n",
    "            sched_arr_date date,\n",
    "            sched_arr_week_day text,\n",
    "            sched_arr_time time,\n",
    "            act_arr_time time,\n",
    "            arrive_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS departures CASCADE;\n",
    "        CREATE TABLE departures (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num int REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_dep_datetime timestamp,\n",
    "            sched_dep_date date,\n",
    "            sched_dep_week_day text,\n",
    "            sched_dep_time time,\n",
    "            act_dep_time time,\n",
    "            depart_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql \n",
    "\n",
    "def update_train_info_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO train_info (train_num, operating_direction, reg_operates_on_mon, \n",
    "                   reg_operates_on_tues, reg_operates_on_wed, reg_operates_on_thurs, \n",
    "                   reg_operates_on_fri, reg_operates_on_sat, reg_operates_on_sun, \n",
    "                   depart_origin_time, depart_NY_time, arrive_dest_time)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                \n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "\n",
    "def update_arrive_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO arrivals (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_arr_datetime, sched_arr_date, sched_arr_week_day,\n",
    "                               sched_arr_time, act_arr_time, arrive_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)     # skip header                                                                         \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n",
    "\n",
    "def update_depart_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO departures (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_dep_datetime, sched_dep_date, sched_dep_week_day,\n",
    "                               sched_dep_time, act_dep_time, depart_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)   # skip header                                                                           \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "depart_filestrings_list = []\n",
    "arrive_filestrings_list = []\n",
    "for year in years:\n",
    "    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "    depart_filestrings_list.append(depart_filestring) \n",
    "    arrive_filestrings_list.append(arrive_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)\n",
    "begin_everything = time.time()\n",
    "update_train_info_table(conn, './data/trains/train_nums.csv')\n",
    "for i in range(len(years)):\n",
    "    start = time.time()\n",
    "    arrive_csv = arrive_filestrings_list[i]\n",
    "    depart_csv = depart_filestrings_list[i]\n",
    "    update_arrive_table(conn, arrive_csv)\n",
    "    update_depart_table(conn, depart_csv)\n",
    "    print(\"DONE WITH\", years[i], 'in', time.time() - start)\n",
    "conn.close()\n",
    "\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys \n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = [('2011-01-01','2011-12-31'), # Original dates list used to get all historical data\n",
    "              ('2012-01-01','2012-12-31'),\n",
    "              ('2013-01-01','2013-12-31'),\n",
    "              ('2014-01-01','2014-12-31'),\n",
    "              ('2015-01-01','2015-12-31'),\n",
    "              ('2016-01-01','2016-12-31'),\n",
    "              ('2017-01-01','2017-12-31'),\n",
    "              ('2018-01-01','2018-12-31'),\n",
    "              ('2019-01-01','2019-12-31'),\n",
    "              ('2020-01-01','2020-11-31')]\n",
    "\n",
    "dates_list = [('2011-01-01','2011-12-31'), # New dates list needed for DB\n",
    "              ('2012-01-01','2012-12-31'),\n",
    "              ('2013-01-01','2013-12-31'),\n",
    "              ('2014-01-01','2014-12-31'),\n",
    "              ('2015-01-01','2015-12-31'),\n",
    "              ('2016-01-01','2016-12-31'),\n",
    "              ('2017-01-01','2017-12-31'),\n",
    "              ('2018-01-01','2018-12-31'),\n",
    "              ('2019-01-01','2019-12-31'),\n",
    "              ('2020-01-01','2021-02-18')]\n",
    "\n",
    "locations = ['Boston,MA', 'Providence,RI', 'Kingston,RI', 'New%20London,CT', 'New%20Haven,CT', 'Stamford,CT', \n",
    "             'Manhattan,NY', 'Newark,NJ', 'Trenton,NJ', 'Philadelphia,PA', 'Wilmington,DE', 'Baltimore,MD', \n",
    "             'Baltimore%20BWI%20Airport,MD', 'New%20Carrollton,MD', 'Washington,DC']\n",
    "\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'New_London_CT', 'New_Haven_CT', 'Stamford_CT', \n",
    "             'Manhattan_NY', 'Newark_NJ', 'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE', 'Baltimore_MD', \n",
    "             'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime='\n",
    "\n",
    "for location, filename in zip(locations, location_names_for_files):\n",
    "    print('Running urls for', location)\n",
    "    for startdate, enddate in dates_list:\n",
    "        url = url_base + startdate + 'T00:00:00&endDateTime=' + enddate + 'T00:00:00&unitGroup=us&contentType=csv&location=' + location + '&key='+os.environ.get('VC_TOKEN')\n",
    "        csv_bytes = requests.get(url).content\n",
    "        filestring = './data/weather_original/' + filename + '_weather_data_' + startdate + '_' + enddate + '.csv'\n",
    "        with open(filestring, 'w', newline='\\n') as csvfile:\n",
    "            csvfile.write(csv_bytes.decode())\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = [('2020-01-01','2020-11-31'), \n",
    "              ('2020-12-01', '2021-02-18')] # replace w curr date\n",
    "for location in location_names_for_files:\n",
    "    print('Fixing data for ', location)\n",
    "    startdate1, enddate1 = dates_list[0]\n",
    "    startdate2, enddate2 = dates_list[1]\n",
    "    weather_2020_part1 = pd.read_csv('./data/weather_original/' + filename + '_weather_data_' + startdate1 + '_' + enddate1 + '.csv')\n",
    "    weather_2020_2021_part2 = pd.read_csv('./data/weather_original/' + filename + '_weather_data_' + startdate2 + '_' + enddate2 + '.csv')\n",
    "    full_weather = pd.concat([weather_2020_part1, weather_2020_2021_part2], ignore_index=True, axis=0)\n",
    "    full_weather_new = full_weather[['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                    'Latitude', 'Longitude', 'Conditions']].iloc[:]\n",
    "    nona_df = full_weather_new.replace('', np.nan).dropna()\n",
    "    print(nona_df.shape[0]/full_weather_new.shape[0])\n",
    "    full_weather_new.to_csv('./data/weather/' +  location + '_weather_data_' + startdate1 + '_' + enddate2 + '_col_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw weather data comes well-formatted in CSV already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filestring = './data/weather_original/Boston_MA_weather_data_2011-01-01_2011-12-31.csv'\n",
    "df_sample = pd.read_csv(filestring)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NA values (very few rows are actually dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in location_names_for_files:\n",
    "    for startdate, enddate in dates_list:\n",
    "        filestring = './data/weather_original/' + location + '_weather_data_' + startdate + '_' + enddate + '.csv'\n",
    "        df = pd.read_csv(filestring, usecols=['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                    'Latitude', 'Longitude', 'Conditions'])\n",
    "        nona_df = df.replace('', np.nan).dropna()\n",
    "        print(nona_df.shape[0]/df.shape[0])\n",
    "        nona_df.to_csv('./data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_table(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "        CREATE TABLE weather_hourly (\n",
    "            weather_id SERIAL PRIMARY KEY,\n",
    "            location text DEFAULT NULL,\n",
    "            date_time timestamp DEFAULT NULL,\n",
    "            temperature real DEFAULT NUll,\n",
    "            precipitation real DEFAULT NULL,\n",
    "            cloud_cover real DEFAULT NULL,\n",
    "            latitude real DEFAULT NULL,\n",
    "            longitude real DEFAULT NULL,\n",
    "            conditions text DEFAULT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_weather_table(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weather_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO weather_hourly (location, date_time, temperature, precipitation, \n",
    "                   cloud_cover, latitude, longitude, conditions)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                   \n",
    "    with open(csv_file, newline='') as file: \n",
    "        data_reader = csv.reader(file, delimiter=',')\n",
    "        next(data_reader, None)   # skip header                                                                           \n",
    "        for row in data_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for startdate, enddate in dates_list:\n",
    "        csv_file = './data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv'\n",
    "        update_weather_table(conn, csv_file)\n",
    "    print('Finished adding location', location, 'to the database')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For fixing the location names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_table(conn):\n",
    "    \"\"\"Create a test table in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS test_table CASCADE;\n",
    "        CREATE TABLE test_table (\n",
    "            weather_id SERIAL PRIMARY KEY,\n",
    "            location text DEFAULT NULL,\n",
    "            date_time timestamp DEFAULT NULL,\n",
    "            temperature real DEFAULT NUll,\n",
    "            precipitation real DEFAULT NULL,\n",
    "            cloud_cover real DEFAULT NULL,\n",
    "            latitude real DEFAULT NULL,\n",
    "            longitude real DEFAULT NULL,\n",
    "            conditions text DEFAULT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO test_table (location, date_time, temperature, precipitation, \n",
    "                   cloud_cover, latitude, longitude, conditions)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                   \n",
    "    with open(csv_file, newline='') as file: \n",
    "        data_reader = csv.reader(file, delimiter=',')\n",
    "        next(data_reader, None)   # skip header                                                                           \n",
    "        for row in data_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_test_table(conn)\n",
    "for location in ['Boston_MA']:\n",
    "    start = time.time()\n",
    "    for startdate, enddate in dates_list:\n",
    "        csv_file = './data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv'\n",
    "        update_test_table(conn, csv_file)\n",
    "print('Finished adding location', location, 'to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "1327576 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need this part to fix the location names\n",
    "#%%sql\n",
    "\n",
    "#UPDATE weather_hourly\n",
    "#SET location = REPLACE(location, ',' , ', ');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
