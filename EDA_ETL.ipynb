{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and ETL Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "from fetch_data import construct_urls, fetch_data_from_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Helper Functions for loading/requesting and processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(option='from_saved', start=date.today()-timedelta(days=1), end=date.today()):\n",
    "    \"\"\"\n",
    "    Function to retrieve new data from the website for specified dates, or else skip to next step.\n",
    "    \"\"\"\n",
    "    if option == 'request':\n",
    "        northbound = [[66, 82, 86, 88, 94], [132, 150, 160, 162, 164, 166], [168, 170, 172, 174]]\n",
    "        southbound = [[67, 83, 93, 95, 99], [135, 137, 139, 161, 163, 165], [167, 171, 173, 175, 195]]\n",
    "        urls = construct_urls(northbound, southbound, start, end)\n",
    "        data = fetch_data_from_urls(urls)\n",
    "    elif option == 'from_saved':\n",
    "        data = None\n",
    "        print(\"Skip this section and go to part B!\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an option\n",
    "* If `option = 'from_saved'`, go to section on Raw Data.\n",
    "* Otherwise, uncomment the other line and wait for request to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = get_data(option='from_saved')\n",
    "start =  date(2021,3,15)\n",
    "end = date(2021,3,31) \n",
    "data = get_data(option='request', start=start, end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(num):\n",
    "    \"\"\"\n",
    "    Return direction of the train (odd = Southbound, even = Northbound).\n",
    "    \"\"\"\n",
    "    if num % 2 == 0:\n",
    "        return 'Northbound'\n",
    "    else:\n",
    "        return 'Southbound'\n",
    "\n",
    "\n",
    "def get_num(re_match):\n",
    "    \"\"\"\n",
    "    Assuming input contains a match , extract and return the numerical data from input.\n",
    "    \"\"\"\n",
    "    num_match = re.search('(?P<num>[0-9]+)', re_match)\n",
    "    return int(num_match.group('num'))\n",
    "\n",
    "\n",
    "def make_dict_from_cols(col_names):\n",
    "    \"\"\"\n",
    "    Create dictionary from a list of column names\n",
    "    \"\"\"\n",
    "    dictionary = { col_name: [] for col_name in col_names }\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_html_col_names(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Using NYP (station with both arrival times and departure times), \n",
    "    retrieve column names from the HTML table, located in the 2nd row.\n",
    "    \"\"\"\n",
    "    data_list = raw_data[arrive_or_depart]['NYP']\n",
    "    page_content = data_list[0]\n",
    "    doc = lh.fromstring(page_content)\n",
    "    tr_elements = doc.xpath('//tr')\n",
    "    html_col_names = [entry.text_content().strip() for entry in tr_elements[1]]        \n",
    "    return html_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_to_raw_df(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Function to put the raw html data in a dataframe for ease of processing.\n",
    "    \"\"\"\n",
    "    col_names = get_html_col_names(raw_data, arrive_or_depart)\n",
    "    N = 7\n",
    "    data_dict = make_dict_from_cols(['Direction', 'Station'] + col_names)\n",
    "    for station in raw_data[arrive_or_depart].keys():\n",
    "        data_list = raw_data[arrive_or_depart][station]\n",
    "        L = len(data_list)\n",
    "        for i in range(L):\n",
    "            page_content = data_list[i]\n",
    "            doc = lh.fromstring(page_content)\n",
    "            tr_elements = doc.xpath('//tr')\n",
    "            if len(tr_elements) > 3:\n",
    "                title = tr_elements[0].text_content()\n",
    "                direction = get_direction(get_num(title))\n",
    "                for j in range(2, len(tr_elements)):\n",
    "                    table_row = tr_elements[j] \n",
    "                    if len(table_row) == N:\n",
    "                        data_dict['Direction'].append(direction)\n",
    "                        data_dict['Station'].append(station)\n",
    "                        for col_name, entry in zip(col_names, table_row):\n",
    "                            data = entry.text_content()\n",
    "                            data_dict[col_name].append(data)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "            else:\n",
    "                print(\"Potentially no data for this time period, or an error occurred\", station, arrive_or_depart)\n",
    "    return pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw data to better format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "depart =  raw_data_to_raw_df(data, 'Depart')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive = data['Arrive']\n",
    "start_time = time.time()\n",
    "arrive = raw_data_to_raw_df(data, 'Arrive')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arrive_filestring = './data/trains/raw_arrive_' + str(start) + '_' + str(end) + '.csv'\n",
    "depart_filestring = './data/trains/raw_depart_' + str(start) + '_' +  str(end) + '.csv'\n",
    "print(arrive_filestring)\n",
    "print(depart_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Train Data - Scraped and Loaded into Pandas DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scraped from an HTML table, so the raw data doesn't look nice after scraping until it's put back in a dataframe. The data was then processed into an initial dataframe and saved as a CSV for later processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = pd.read_csv(arrive_filestring, lineterminator='\\n', keep_default_na=False)\n",
    "depart = pd.read_csv(depart_filestring, lineterminator='\\n', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return ['Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Ar Date', 'Sch Ar Date', \n",
    "                 'Sch Ar Day', 'Sch Ar Time','Act Ar Time', 'Arrive Diff', 'Service Disruption', 'Cancellations']\n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return [ 'Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Dp Date','Sch Dp Date', \n",
    "                 'Sch Dp Day', 'Sch Dp Time','Act Dp Time', 'Depart Diff', 'Service Disruption', 'Cancellations']\n",
    "\n",
    "    \n",
    "def get_key_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return {'Sch Full Date': 'Full Sch Ar Date', 'Sch Abbr': 'Sch Ar', 'Act Abbr': 'Act Ar', 'Diff': 'Arrive Diff'}\n",
    "    \n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return {'Sch Full Date': 'Full Sch Dp Date', 'Sch Abbr': 'Sch Dp', 'Act Abbr': 'Act Dp', 'Diff': 'Depart Diff'}\n",
    "\n",
    "\n",
    "def process_columns(df, arrive_or_depart):\n",
    "    ad_keys = get_key_names(arrive_or_depart) # the specific keys depending on if new_df is for arr or dep data\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Train Num'] = pd.to_numeric(df['Train #'])\n",
    "    new_df['Station'] = df['Station']\n",
    "    new_df['Direction'] = df['Direction']\n",
    "    \n",
    "    origin_date = pd.to_datetime(df['Origin Date'], format=\"%m/%d/%Y\", exact=False, errors='coerce')    \n",
    "    new_df['Origin Date'] = origin_date\n",
    "    new_df['Origin Year'] = origin_date.dt.year\n",
    "    new_df['Origin Quarter'] = origin_date.dt.quarter\n",
    "    new_df['Origin Month'] = origin_date.dt.month\n",
    "    new_df['Origin Day'] = origin_date.dt.day\n",
    "    new_df['Origin Week Day'] = origin_date.dt.day_name()\n",
    "    \n",
    "    sched_full_date = pd.to_datetime(df[ad_keys['Sch Abbr']], format='%m/%d/%Y %I:%M %p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Sch Full Date']] = sched_full_date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Date'] = sched_full_date.dt.date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Day'] = sched_full_date.dt.day_name()\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Time'] = sched_full_date.dt.time\n",
    "    act_time = pd.to_datetime(df[ad_keys['Act Abbr']], format='%I:%M%p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Act Abbr'] + ' Time'] = act_time.dt.time\n",
    "    \n",
    "    df['Sched Date'] = sched_full_date \n",
    "    df['Act Date'] = pd.to_datetime(sched_full_date.dt.date.astype(str) + \" \" + df[ad_keys['Act Abbr']].astype(str),exact=False, errors='coerce')\n",
    "    max_expected_delay = pd.Timedelta(hours=10)\n",
    "    delta = df['Act Date'] - df['Sched Date']\n",
    "    m_late = (delta < max_expected_delay) & (-1*max_expected_delay > delta)\n",
    "    m_early = (-1*delta < max_expected_delay) & (-1*max_expected_delay > -1*delta)\n",
    "    df.loc[m_late, 'Act Date'] += pd.Timedelta(days=1)\n",
    "    df.loc[m_early, 'Act Date'] -= pd.Timedelta(days=1)\n",
    "    new_df[ad_keys['Diff']] = np.rint((df['Act Date'] - df['Sched Date']).dt.total_seconds()/60).astype(int)\n",
    "    new_df['Service Disruption'] = df['Service Disruption'].replace('SD', 1).replace('', 0)\n",
    "    new_df['Cancellations'] =  df['Cancellations'].replace('C', 1).replace('', 0)\n",
    "    return new_df.replace('', np.nan).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_arrive.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files by year to break down data into smaller chunks\n",
    "* Ignore any data from 2010, this is only 23 rows in the departure and arrival dataframes combined (due to trains that were retrieved with the web request starting 1/1/2011 but originated in 2010). \n",
    "* Subset into files by arrival and departure by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "#for year in years:\n",
    "#    depart_subset = full_depart.loc[(full_depart['Origin Year'] == year)]\n",
    "#    arrive_subset = full_arrive.loc[(full_arrive['Origin Year'] == year)]\n",
    "#    print(depart_subset.shape[0], arrive_subset.shape[0])\n",
    "#    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "#    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "#    depart_subset.to_csv(depart_filestring, line_terminator='\\n', index=False)\n",
    "#    arrive_subset.to_csv(arrive_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_arrive_2020 = pd.read_csv('./data/trains/processed_arrive_2020.csv')\n",
    "#prev_depart_2020 = pd.read_csv('./data/trains/processed_depart_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_depart_2020['Origin Month'].unique()\n",
    "#prev_arrive_2020['Origin Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_depart_2020['Sch Dp Date'].unique().shape[0] \n",
    "\n",
    "# 2020 was a leap year !!!\n",
    "# and it's 366 + 1 = 367 because the overnight train starts in 2020 and goes until 2021 \n",
    "# fun times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arrive_2021 = pd.read_csv('./data/trains/processed_arrive_2021.csv')\n",
    "prev_depart_2021 = pd.read_csv('./data/trains/processed_depart_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive_2021 = pd.concat([prev_arrive_2021, full_arrive], ignore_index=True, axis=0)\n",
    "new_depart_2021 = pd.concat([prev_depart_2021, full_depart], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart_2021['Sch Dp Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive_2021.to_csv('./data/trains/processed_arrive_2021.csv', line_terminator='\\n', index=False)\n",
    "new_depart_2021.to_csv('./data/trains/processed_depart_2021.csv', line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Postgres Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE TO SELF: RUN IF USING INTEL CONDA ENV\n",
    "## username is \"appuser\" # no longer\n",
    "# DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "# conn = psycopg2.connect(DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN IF USING arm64 CONDA ENV\n",
    "## username is \"ecc\"\n",
    "# DSN = \"dbname='amtrakproject' user='ecc' password={}\".format(os.environ.get('DB_PASS'))\n",
    "#conn = psycopg2.connect(DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN IF FOR HEROKU\n",
    "assert os.environ.get('DATABASE_URL') != None, 'need to set DATABASE_URL config var'\n",
    "DATABASE_URL = os.environ['DATABASE_URL']\n",
    "conn = psycopg2.connect(DATABASE_URL, sslmode='require')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_tables(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS train_info CASCADE;\n",
    "        CREATE TABLE train_info (\n",
    "            train_info_id SERIAL PRIMARY KEY,\n",
    "            train_num text UNIQUE,\n",
    "            operating_direction text,\n",
    "            reg_operates_on_mon boolean,\n",
    "            reg_operates_on_tues boolean,\n",
    "            reg_operates_on_wed boolean,\n",
    "            reg_operates_on_thurs boolean,\n",
    "            reg_operates_on_fri boolean,\n",
    "            reg_operates_on_sat boolean,\n",
    "            reg_operates_on_sun boolean,\n",
    "            depart_origin_time text,\n",
    "            depart_NY_time text,\n",
    "            arrive_dest_time text\n",
    "            \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS arrivals CASCADE;\n",
    "        CREATE TABLE arrivals (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num text REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_arr_datetime timestamp,\n",
    "            sched_arr_date date,\n",
    "            sched_arr_week_day text,\n",
    "            sched_arr_time time,\n",
    "            act_arr_time time,\n",
    "            arrive_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS departures CASCADE;\n",
    "        CREATE TABLE departures (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num text REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_dep_datetime timestamp,\n",
    "            sched_dep_date date,\n",
    "            sched_dep_week_day text,\n",
    "            sched_dep_time time,\n",
    "            act_dep_time time,\n",
    "            depart_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_tables(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql \n",
    "\n",
    "def update_train_info_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO train_info (train_num, operating_direction, reg_operates_on_mon, \n",
    "                   reg_operates_on_tues, reg_operates_on_wed, reg_operates_on_thurs, \n",
    "                   reg_operates_on_fri, reg_operates_on_sat, reg_operates_on_sun, \n",
    "                   depart_origin_time, depart_NY_time, arrive_dest_time)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                \n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "\n",
    "def update_arrive_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO arrivals (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_arr_datetime, sched_arr_date, sched_arr_week_day,\n",
    "                               sched_arr_time, act_arr_time, arrive_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)     # skip header                                                                         \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n",
    "\n",
    "def update_depart_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO departures (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_dep_datetime, sched_dep_date, sched_dep_week_day,\n",
    "                               sched_dep_time, act_dep_time, depart_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)   # skip header                                                                           \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "depart_filestrings_list = []\n",
    "arrive_filestrings_list = []\n",
    "for year in years:\n",
    "    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "    depart_filestrings_list.append(depart_filestring) \n",
    "    arrive_filestrings_list.append(arrive_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/trains/processed_depart_2011.csv', './data/trains/processed_depart_2012.csv', './data/trains/processed_depart_2013.csv', './data/trains/processed_depart_2014.csv', './data/trains/processed_depart_2015.csv', './data/trains/processed_depart_2016.csv', './data/trains/processed_depart_2017.csv', './data/trains/processed_depart_2018.csv', './data/trains/processed_depart_2019.csv', './data/trains/processed_depart_2020.csv', './data/trains/processed_depart_2021.csv']\n",
      "['./data/trains/processed_arrive_2011.csv', './data/trains/processed_arrive_2012.csv', './data/trains/processed_arrive_2013.csv', './data/trains/processed_arrive_2014.csv', './data/trains/processed_arrive_2015.csv', './data/trains/processed_arrive_2016.csv', './data/trains/processed_arrive_2017.csv', './data/trains/processed_arrive_2018.csv', './data/trains/processed_arrive_2019.csv', './data/trains/processed_arrive_2020.csv', './data/trains/processed_arrive_2021.csv']\n"
     ]
    }
   ],
   "source": [
    "print(depart_filestrings_list)\n",
    "print(arrive_filestrings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH 2011 in 3.4802911281585693\n",
      "DONE WITH 2012 in 3.4518070220947266\n",
      "DONE WITH 2013 in 3.6265180110931396\n",
      "DONE WITH 2014 in 3.7650251388549805\n",
      "DONE WITH 2015 in 3.7895209789276123\n",
      "DONE WITH 2016 in 3.8809731006622314\n",
      "DONE WITH 2017 in 3.8615291118621826\n",
      "DONE WITH 2018 in 3.7825160026550293\n",
      "DONE WITH 2019 in 3.8574459552764893\n",
      "DONE WITH 2020 in 5.321774959564209\n",
      "DONE WITH 2021 in 0.8470830917358398\n",
      "COMPLETE in 39.67019510269165\n"
     ]
    }
   ],
   "source": [
    "## Ran on Apple Silicon arch\n",
    "## New output\n",
    "import time\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_train_tables(conn)\n",
    "\n",
    "begin_everything = time.time()\n",
    "update_train_info_table(conn, './data/trains/train_nums.csv')\n",
    "for i in range(len(years)):\n",
    "    start = time.time()\n",
    "    arrive_csv = arrive_filestrings_list[i]\n",
    "    depart_csv = depart_filestrings_list[i]\n",
    "    update_arrive_table(conn, arrive_csv)\n",
    "    update_depart_table(conn, depart_csv)\n",
    "    print('DONE WITH', years[i], 'in', time.time() - start)\n",
    "conn.close()\n",
    "\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://ecc:test@localhost:5432/amtrakproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://ecc:***@localhost:5432/amtrakproject\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>791497</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(791497,)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) from departures;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://ecc:***@localhost:5432/amtrakproject\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>104047</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(104047,)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) from arrivals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train rows presently 4/3 895544\n"
     ]
    }
   ],
   "source": [
    "print('num train rows presently 4/3', 104047 + 791497)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num weather rows presently 4/3 1347202\n"
     ]
    }
   ],
   "source": [
    "print('num weather rows presently 4/3', 1347202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows presntly 4/3 2242746\n"
     ]
    }
   ],
   "source": [
    "print('total rows presntly 4/3', 104047 + 791497 + 1347202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num yearly added rows 198640\n"
     ]
    }
   ],
   "source": [
    "num_weekly_train =  1300\n",
    "num_weekly_weather = 15 * 24 * 7\n",
    "num_yearly = 52 * (num_weekly_train + num_weekly_weather)\n",
    "print('num yearly added rows', num_yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH 2011 in 5.648728132247925\n",
      "DONE WITH 2012 in 5.568975925445557\n",
      "DONE WITH 2013 in 5.822044849395752\n",
      "DONE WITH 2014 in 6.143963813781738\n",
      "DONE WITH 2015 in 6.068666934967041\n",
      "DONE WITH 2016 in 6.223670959472656\n",
      "DONE WITH 2017 in 6.284543991088867\n",
      "DONE WITH 2018 in 6.0637428760528564\n",
      "DONE WITH 2019 in 6.20758581161499\n",
      "DONE WITH 2020 in 8.508411169052124\n",
      "DONE WITH 2021 in 1.3244810104370117\n",
      "COMPLETE in 63.870940923690796\n"
     ]
    }
   ],
   "source": [
    "## Ran on Intel emulated arch\n",
    "## Old output\n",
    "\n",
    "#import time\n",
    "\n",
    "#years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "#conn = psycopg2.connect(DSN)\n",
    "#create_tables(conn)\n",
    "\n",
    "#begin_everything = time.time()\n",
    "#update_train_info_table(conn, './data/trains/train_nums.csv')\n",
    "#for i in range(len(years)):\n",
    "#    start = time.time()\n",
    "#    arrive_csv = arrive_filestrings_list[i]\n",
    "#    depart_csv = depart_filestrings_list[i]\n",
    "#    update_arrive_table(conn, arrive_csv)\n",
    "#    update_depart_table(conn, depart_csv)\n",
    "#    print(\"DONE WITH\", years[i], 'in', time.time() - start)\n",
    "#conn.close()\n",
    "\n",
    "#print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys \n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['Boston,MA', 'Providence,RI', 'Kingston,RI', 'New%20London,CT', 'New%20Haven,CT', 'Stamford,CT', \n",
    "             'Manhattan,NY', 'Newark,NJ', 'Trenton,NJ', 'Philadelphia,PA', 'Wilmington,DE', 'Baltimore,MD', \n",
    "             'Baltimore%20BWI%20Airport,MD', 'New%20Carrollton,MD', 'Washington,DC']\n",
    "\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'New_London_CT', 'New_Haven_CT', \n",
    "                            'Stamford_CT', 'Manhattan_NY', 'Newark_NJ', 'Trenton_NJ', 'Philadelphia_PA', \n",
    "                            'Wilmington_DE', 'Baltimore_MD', 'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', \n",
    "                            'Washington_DC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_DATES_LIST = \"\"\"\n",
    "dates_list = [('2011-01-01','2012-01-01'), \n",
    "              ('2012-01-01','2013-01-01'),\n",
    "              ('2013-01-01','2014-01-01'),\n",
    "              ('2014-01-01','2015-01-01'),\n",
    "              ('2015-01-01','2016-01-01'),\n",
    "              ('2016-01-01','2017-01-01'),\n",
    "              ('2017-01-01','2018-01-01'),\n",
    "              ('2018-01-01','2019-01-01'),\n",
    "              ('2019-01-01','2020-01-01'),\n",
    "              ('2020-01-01','2021-01-01')]\"\"\"\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "dates_list = [('2021-04-02', '2021-04-04')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELLO! Uncomment the requests.get() line when you have quadruple checked the dates!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running urls for Boston,MA\n",
      "Running urls for Providence,RI\n",
      "Running urls for Kingston,RI\n",
      "Running urls for New%20London,CT\n",
      "Running urls for New%20Haven,CT\n",
      "Running urls for Stamford,CT\n",
      "Running urls for Manhattan,NY\n",
      "Running urls for Newark,NJ\n",
      "Running urls for Trenton,NJ\n",
      "Running urls for Philadelphia,PA\n",
      "Running urls for Wilmington,DE\n",
      "Running urls for Baltimore,MD\n",
      "Running urls for Baltimore%20BWI%20Airport,MD\n",
      "Running urls for New%20Carrollton,MD\n",
      "Running urls for Washington,DC\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime='\n",
    "\n",
    "for location, filename in zip(locations, location_names_for_files):\n",
    "    print('Running urls for', location)\n",
    "    for startdate, enddate in dates_list:\n",
    "        url = url_base + startdate + 'T00:00:00&endDateTime=' + enddate + 'T00:00:00&unitGroup=us&contentType=csv&location=' + location + '&key='+os.environ.get('VC_TOKEN')\n",
    "    #    csv_bytes = requests.get(url).content\n",
    "        filestring = './data/weather_original/' + filename + '_weather_data_' + startdate + '_' + enddate + '.csv'\n",
    "        with open(filestring, 'w', newline='\\n') as csvfile:\n",
    "            csvfile.write(csv_bytes.decode())\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW! Re-comment the line out so you don't make a mistake next time!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning process\n",
    "* Dropping NA values (very small fraction actually dropped)\n",
    "* Rename city name to include a space after the comma \n",
    "* Select chosen columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston_MA 2011 fraction of data kept:  0.9996575342465753\n",
      "Boston_MA 2012 fraction of data kept:  0.9995446265938069\n",
      "Boston_MA 2013 fraction of data kept:  1.0\n",
      "Boston_MA 2014 fraction of data kept:  1.0\n",
      "Boston_MA 2015 fraction of data kept:  1.0\n",
      "Boston_MA 2016 fraction of data kept:  0.9994307832422586\n",
      "Boston_MA 2017 fraction of data kept:  0.9997717155575847\n",
      "Boston_MA 2018 fraction of data kept:  0.9997716894977169\n",
      "Boston_MA 2019 fraction of data kept:  0.9998858577787924\n",
      "Boston_MA 2020 fraction of data kept:  1.0\n",
      "Providence_RI 2011 fraction of data kept:  0.9992009132420091\n",
      "Providence_RI 2012 fraction of data kept:  0.9992030965391621\n",
      "Providence_RI 2013 fraction of data kept:  0.9998858577787924\n",
      "Providence_RI 2014 fraction of data kept:  0.9958904109589041\n",
      "Providence_RI 2015 fraction of data kept:  0.9993151466727542\n",
      "Providence_RI 2016 fraction of data kept:  0.9990892531876139\n",
      "Providence_RI 2017 fraction of data kept:  0.9965757333637713\n",
      "Providence_RI 2018 fraction of data kept:  0.9994292237442922\n",
      "Providence_RI 2019 fraction of data kept:  1.0\n",
      "Providence_RI 2020 fraction of data kept:  1.0\n",
      "Kingston_RI 2011 fraction of data kept:  0.9995434311151695\n",
      "Kingston_RI 2012 fraction of data kept:  0.9997723392145703\n",
      "Kingston_RI 2013 fraction of data kept:  0.9997717155575847\n",
      "Kingston_RI 2014 fraction of data kept:  0.9995434311151695\n",
      "Kingston_RI 2015 fraction of data kept:  0.9998858577787924\n",
      "Kingston_RI 2016 fraction of data kept:  1.0\n",
      "Kingston_RI 2017 fraction of data kept:  0.9998858577787924\n",
      "Kingston_RI 2018 fraction of data kept:  1.0\n",
      "Kingston_RI 2019 fraction of data kept:  0.9998858577787924\n",
      "Kingston_RI 2020 fraction of data kept:  1.0\n",
      "New_London_CT 2011 fraction of data kept:  0.9968036529680365\n",
      "New_London_CT 2012 fraction of data kept:  0.9998861696072852\n",
      "New_London_CT 2013 fraction of data kept:  1.0\n",
      "New_London_CT 2014 fraction of data kept:  1.0\n",
      "New_London_CT 2015 fraction of data kept:  0.9997717155575847\n",
      "New_London_CT 2016 fraction of data kept:  1.0\n",
      "New_London_CT 2017 fraction of data kept:  0.9974888711334322\n",
      "New_London_CT 2018 fraction of data kept:  0.9990867579908675\n",
      "New_London_CT 2019 fraction of data kept:  1.0\n",
      "New_London_CT 2020 fraction of data kept:  1.0\n",
      "New_Haven_CT 2011 fraction of data kept:  0.9982876712328768\n",
      "New_Haven_CT 2012 fraction of data kept:  0.9998861696072852\n",
      "New_Haven_CT 2013 fraction of data kept:  1.0\n",
      "New_Haven_CT 2014 fraction of data kept:  0.9930365296803653\n",
      "New_Haven_CT 2015 fraction of data kept:  0.992579908675799\n",
      "New_Haven_CT 2016 fraction of data kept:  0.9996584699453552\n",
      "New_Haven_CT 2017 fraction of data kept:  0.9995434311151695\n",
      "New_Haven_CT 2018 fraction of data kept:  0.9963470319634703\n",
      "New_Haven_CT 2019 fraction of data kept:  0.9997717155575847\n",
      "New_Haven_CT 2020 fraction of data kept:  0.998747723132969\n",
      "Stamford_CT 2011 fraction of data kept:  0.9993151466727542\n",
      "Stamford_CT 2012 fraction of data kept:  0.9973816029143898\n",
      "Stamford_CT 2013 fraction of data kept:  0.9994292237442922\n",
      "Stamford_CT 2014 fraction of data kept:  0.9990867579908675\n",
      "Stamford_CT 2015 fraction of data kept:  0.9985161511243009\n",
      "Stamford_CT 2016 fraction of data kept:  0.9993169398907104\n",
      "Stamford_CT 2017 fraction of data kept:  0.9987444355667161\n",
      "Stamford_CT 2018 fraction of data kept:  0.9995433789954338\n",
      "Stamford_CT 2019 fraction of data kept:  0.9995434311151695\n",
      "Stamford_CT 2020 fraction of data kept:  0.999203187250996\n",
      "Manhattan_NY 2011 fraction of data kept:  0.9961191644789408\n",
      "Manhattan_NY 2012 fraction of data kept:  1.0\n",
      "Manhattan_NY 2013 fraction of data kept:  1.0\n",
      "Manhattan_NY 2014 fraction of data kept:  1.0\n",
      "Manhattan_NY 2015 fraction of data kept:  0.9905251141552511\n",
      "Manhattan_NY 2016 fraction of data kept:  1.0\n",
      "Manhattan_NY 2017 fraction of data kept:  0.9996575733363772\n",
      "Manhattan_NY 2018 fraction of data kept:  0.9997717155575847\n",
      "Manhattan_NY 2019 fraction of data kept:  1.0\n",
      "Manhattan_NY 2020 fraction of data kept:  0.9997723392145703\n",
      "Newark_NJ 2011 fraction of data kept:  0.9998858447488584\n",
      "Newark_NJ 2012 fraction of data kept:  0.9997723132969034\n",
      "Newark_NJ 2013 fraction of data kept:  1.0\n",
      "Newark_NJ 2014 fraction of data kept:  1.0\n",
      "Newark_NJ 2015 fraction of data kept:  1.0\n",
      "Newark_NJ 2016 fraction of data kept:  1.0\n",
      "Newark_NJ 2017 fraction of data kept:  1.0\n",
      "Newark_NJ 2018 fraction of data kept:  0.9998858447488584\n",
      "Newark_NJ 2019 fraction of data kept:  0.9998858577787924\n",
      "Newark_NJ 2020 fraction of data kept:  1.0\n",
      "Trenton_NJ 2011 fraction of data kept:  0.9998858577787924\n",
      "Trenton_NJ 2012 fraction of data kept:  0.9998861696072852\n",
      "Trenton_NJ 2013 fraction of data kept:  1.0\n",
      "Trenton_NJ 2014 fraction of data kept:  1.0\n",
      "Trenton_NJ 2015 fraction of data kept:  0.9992010044515466\n",
      "Trenton_NJ 2016 fraction of data kept:  0.9985200364298725\n",
      "Trenton_NJ 2017 fraction of data kept:  0.9997717155575847\n",
      "Trenton_NJ 2018 fraction of data kept:  0.9990867579908675\n",
      "Trenton_NJ 2019 fraction of data kept:  0.9998858577787924\n",
      "Trenton_NJ 2020 fraction of data kept:  0.993739328400683\n",
      "Philadelphia_PA 2011 fraction of data kept:  0.9996575342465753\n",
      "Philadelphia_PA 2012 fraction of data kept:  1.0\n",
      "Philadelphia_PA 2013 fraction of data kept:  0.9993150684931507\n",
      "Philadelphia_PA 2014 fraction of data kept:  0.9998858447488584\n",
      "Philadelphia_PA 2015 fraction of data kept:  0.9998858447488584\n",
      "Philadelphia_PA 2016 fraction of data kept:  1.0\n",
      "Philadelphia_PA 2017 fraction of data kept:  0.9998858447488584\n",
      "Philadelphia_PA 2018 fraction of data kept:  1.0\n",
      "Philadelphia_PA 2019 fraction of data kept:  0.9992009132420091\n",
      "Philadelphia_PA 2020 fraction of data kept:  0.9997723132969034\n",
      "Wilmington_DE 2011 fraction of data kept:  0.9988584474885844\n",
      "Wilmington_DE 2012 fraction of data kept:  0.9996584699453552\n",
      "Wilmington_DE 2013 fraction of data kept:  0.9909817351598174\n",
      "Wilmington_DE 2014 fraction of data kept:  1.0\n",
      "Wilmington_DE 2015 fraction of data kept:  0.9998858577787924\n",
      "Wilmington_DE 2016 fraction of data kept:  0.9985200364298725\n",
      "Wilmington_DE 2017 fraction of data kept:  0.9998858577787924\n",
      "Wilmington_DE 2018 fraction of data kept:  1.0\n",
      "Wilmington_DE 2019 fraction of data kept:  0.9952054794520548\n",
      "Wilmington_DE 2020 fraction of data kept:  0.9996585088218555\n",
      "Baltimore_MD 2011 fraction of data kept:  1.0\n",
      "Baltimore_MD 2012 fraction of data kept:  0.9994307832422586\n",
      "Baltimore_MD 2013 fraction of data kept:  1.0\n",
      "Baltimore_MD 2014 fraction of data kept:  1.0\n",
      "Baltimore_MD 2015 fraction of data kept:  0.9987444355667161\n",
      "Baltimore_MD 2016 fraction of data kept:  0.9952185792349727\n",
      "Baltimore_MD 2017 fraction of data kept:  0.9948636000456569\n",
      "Baltimore_MD 2018 fraction of data kept:  0.9997716894977169\n",
      "Baltimore_MD 2019 fraction of data kept:  0.999086862230339\n",
      "Baltimore_MD 2020 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2011 fraction of data kept:  0.9998858447488584\n",
      "Baltimore_BWI_Airport_MD 2012 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2013 fraction of data kept:  0.9998858447488584\n",
      "Baltimore_BWI_Airport_MD 2014 fraction of data kept:  0.9997716894977169\n",
      "Baltimore_BWI_Airport_MD 2015 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2016 fraction of data kept:  0.9996584699453552\n",
      "Baltimore_BWI_Airport_MD 2017 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2018 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2019 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2020 fraction of data kept:  1.0\n",
      "New_Carrollton_MD 2011 fraction of data kept:  0.9865312178975003\n",
      "New_Carrollton_MD 2012 fraction of data kept:  0.9838342440801457\n",
      "New_Carrollton_MD 2013 fraction of data kept:  0.999086862230339\n",
      "New_Carrollton_MD 2014 fraction of data kept:  0.999086862230339\n",
      "New_Carrollton_MD 2015 fraction of data kept:  0.9985161511243009\n",
      "New_Carrollton_MD 2016 fraction of data kept:  1.0\n",
      "New_Carrollton_MD 2017 fraction of data kept:  0.9920091324200914\n",
      "New_Carrollton_MD 2018 fraction of data kept:  1.0\n",
      "New_Carrollton_MD 2019 fraction of data kept:  0.9993150684931507\n",
      "New_Carrollton_MD 2020 fraction of data kept:  0.9981787137165623\n",
      "Washington_DC 2011 fraction of data kept:  0.9992009132420091\n",
      "Washington_DC 2012 fraction of data kept:  0.9998861566484517\n",
      "Washington_DC 2013 fraction of data kept:  1.0\n",
      "Washington_DC 2014 fraction of data kept:  1.0\n",
      "Washington_DC 2015 fraction of data kept:  0.9997717155575847\n",
      "Washington_DC 2016 fraction of data kept:  0.9993169398907104\n",
      "Washington_DC 2017 fraction of data kept:  0.9998858577787924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington_DC 2018 fraction of data kept:  0.9997716894977169\n",
      "Washington_DC 2019 fraction of data kept:  1.0\n",
      "Washington_DC 2020 fraction of data kept:  1.0\n"
     ]
    }
   ],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for location in location_names_for_files:\n",
    "    for year in years:\n",
    "        filename = './data/weather_original/' + location + '_weather_' + str(year) + '.csv'\n",
    "        full_weather = pd.read_csv(filename)\n",
    "        full_weather_new = full_weather[['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                    'Latitude', 'Longitude', 'Conditions']]\n",
    "        dropna_weather = full_weather_new.replace('', np.nan).dropna()\n",
    "        dropna_weather['Address'] = dropna_weather['Address'].str.replace(',', ', ')\n",
    "        print(location, year, 'fraction of data kept: ', dropna_weather.shape[0]/full_weather_new.shape[0])\n",
    "        dropna_weather.to_csv('./data/weather/' + location + '_weather_' + str(year) + '_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston_MA 2020 fraction of data kept:  1.0\n",
      "Providence_RI 2020 fraction of data kept:  1.0\n",
      "Kingston_RI 2020 fraction of data kept:  1.0\n",
      "New_London_CT 2020 fraction of data kept:  1.0\n",
      "New_Haven_CT 2020 fraction of data kept:  1.0\n",
      "Stamford_CT 2020 fraction of data kept:  1.0\n",
      "Manhattan_NY 2020 fraction of data kept:  1.0\n",
      "Newark_NJ 2020 fraction of data kept:  1.0\n",
      "Trenton_NJ 2020 fraction of data kept:  1.0\n",
      "Philadelphia_PA 2020 fraction of data kept:  1.0\n",
      "Wilmington_DE 2020 fraction of data kept:  1.0\n",
      "Baltimore_MD 2020 fraction of data kept:  1.0\n",
      "Baltimore_BWI_Airport_MD 2020 fraction of data kept:  1.0\n",
      "New_Carrollton_MD 2020 fraction of data kept:  1.0\n",
      "Washington_DC 2020 fraction of data kept:  0.9995511669658886\n"
     ]
    }
   ],
   "source": [
    "for location in location_names_for_files:\n",
    "    filename = './data/weather_original/' + location + '_weather_data_2021-01-01_2021-04-02.csv'\n",
    "    full_weather = pd.read_csv(filename)\n",
    "    full_weather_new = full_weather[['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                'Latitude', 'Longitude', 'Conditions']]\n",
    "    dropna_weather = full_weather_new.replace('', np.nan).dropna()\n",
    "    dropna_weather['Address'] = dropna_weather['Address'].str.replace(',', ', ')\n",
    "    print(location, year, 'fraction of data kept: ', dropna_weather.shape[0]/full_weather_new.shape[0])\n",
    "    dropna_weather.to_csv('./data/weather/' +  location + '_weather_2021_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw weather data comes well-formatted in CSV already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Date time</th>\n",
       "      <th>Minimum Temperature</th>\n",
       "      <th>Maximum Temperature</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Relative Humidity</th>\n",
       "      <th>Heat Index</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Wind Gust</th>\n",
       "      <th>...</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Cloud Cover</th>\n",
       "      <th>Sea Level Pressure</th>\n",
       "      <th>Weather Type</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Resolved Address</th>\n",
       "      <th>Name</th>\n",
       "      <th>Info</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston,MA</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>41.1</td>\n",
       "      <td>41.1</td>\n",
       "      <td>41.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>61.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.3587</td>\n",
       "      <td>-71.0567</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston,MA</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>39.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.9</td>\n",
       "      <td>67.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.3587</td>\n",
       "      <td>-71.0567</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boston,MA</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>37.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>72.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1017.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.3587</td>\n",
       "      <td>-71.0567</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partially cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boston,MA</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>78.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.3587</td>\n",
       "      <td>-71.0567</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partially cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston,MA</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>37.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>29.9</td>\n",
       "      <td>74.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1017.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.3587</td>\n",
       "      <td>-71.0567</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>Boston, MA, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Overcast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Address            Date time  Minimum Temperature  Maximum Temperature  \\\n",
       "0  Boston,MA  2011-01-01 00:00:00                 41.1                 41.1   \n",
       "1  Boston,MA  2011-01-01 01:00:00                 39.8                 39.8   \n",
       "2  Boston,MA  2011-01-01 02:00:00                 37.1                 37.1   \n",
       "3  Boston,MA  2011-01-01 03:00:00                 35.0                 35.0   \n",
       "4  Boston,MA  2011-01-01 04:00:00                 37.1                 37.1   \n",
       "\n",
       "   Temperature  Dew Point  Relative Humidity  Heat Index  Wind Speed  \\\n",
       "0         41.1       29.0              61.93         NaN        13.9   \n",
       "1         39.8       29.9              67.46         NaN        11.4   \n",
       "2         37.1       29.0              72.27         NaN         8.1   \n",
       "3         35.0       29.0              78.72         NaN         5.8   \n",
       "4         37.1       29.9              74.97         NaN         6.9   \n",
       "\n",
       "   Wind Gust  ...  Visibility  Cloud Cover  Sea Level Pressure  Weather Type  \\\n",
       "0        NaN  ...         9.9          0.0              1017.0           NaN   \n",
       "1        NaN  ...         9.9          0.0              1017.0           NaN   \n",
       "2        NaN  ...         8.7         30.0              1017.4           NaN   \n",
       "3        NaN  ...         8.1         30.0              1017.6           NaN   \n",
       "4        NaN  ...         8.7         90.0              1017.5           NaN   \n",
       "\n",
       "   Latitude  Longitude           Resolved Address                       Name  \\\n",
       "0   42.3587   -71.0567  Boston, MA, United States  Boston, MA, United States   \n",
       "1   42.3587   -71.0567  Boston, MA, United States  Boston, MA, United States   \n",
       "2   42.3587   -71.0567  Boston, MA, United States  Boston, MA, United States   \n",
       "3   42.3587   -71.0567  Boston, MA, United States  Boston, MA, United States   \n",
       "4   42.3587   -71.0567  Boston, MA, United States  Boston, MA, United States   \n",
       "\n",
       "  Info        Conditions  \n",
       "0  NaN             Clear  \n",
       "1  NaN             Clear  \n",
       "2  NaN  Partially cloudy  \n",
       "3  NaN  Partially cloudy  \n",
       "4  NaN          Overcast  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filestring = './data/weather_original/Boston_MA_weather_2011.csv'\n",
    "df_sample = pd.read_csv(filestring)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_table(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "        CREATE TABLE weather_hourly (\n",
    "            weather_id SERIAL PRIMARY KEY,\n",
    "            location text DEFAULT NULL,\n",
    "            date_time timestamp DEFAULT NULL,\n",
    "            temperature real DEFAULT NUll,\n",
    "            precipitation real DEFAULT NULL,\n",
    "            cloud_cover real DEFAULT NULL,\n",
    "            latitude real DEFAULT NULL,\n",
    "            longitude real DEFAULT NULL,\n",
    "            conditions text DEFAULT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "DSN = \"dbname='amtrakproject' user='ecc' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_weather_table(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weather_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO weather_hourly (location, date_time, temperature, precipitation, \n",
    "                   cloud_cover, latitude, longitude, conditions)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                   \n",
    "    with open(csv_file, newline='') as file: \n",
    "        data_reader = csv.reader(file, delimiter=',')\n",
    "        next(data_reader, None)   # skip header                                                                           \n",
    "        for row in data_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding location Boston_MA to the database in 2.488786220550537 seconds\n",
      "Finished adding location Providence_RI to the database in 2.4389419555664062 seconds\n",
      "Finished adding location Kingston_RI to the database in 2.436068058013916 seconds\n",
      "Finished adding location New_London_CT to the database in 2.431364059448242 seconds\n",
      "Finished adding location New_Haven_CT to the database in 2.425219774246216 seconds\n",
      "Finished adding location Stamford_CT to the database in 2.478576898574829 seconds\n",
      "Finished adding location Manhattan_NY to the database in 2.423409938812256 seconds\n",
      "Finished adding location Newark_NJ to the database in 2.4260599613189697 seconds\n",
      "Finished adding location Trenton_NJ to the database in 2.422894239425659 seconds\n",
      "Finished adding location Philadelphia_PA to the database in 2.486855983734131 seconds\n",
      "Finished adding location Wilmington_DE to the database in 2.4337100982666016 seconds\n",
      "Finished adding location Baltimore_MD to the database in 2.458324909210205 seconds\n",
      "Finished adding location Baltimore_BWI_Airport_MD to the database in 2.476867198944092 seconds\n",
      "Finished adding location New_Carrollton_MD to the database in 2.489176034927368 seconds\n",
      "Finished adding location Washington_DC to the database in 2.46516489982605 seconds\n",
      "COMPLETE in 36.78361678123474\n"
     ]
    }
   ],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_weather_table(conn)\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        csv_file = './data/weather/' + location + '_weather_' + str(year) + '_subset.csv'\n",
    "        update_weather_table(conn, csv_file)\n",
    "    print('Finished adding location', location, 'to the database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above ^ all data from 2011 - April 2,  2021 was inserted into the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://ecc:test@localhost:5432/amtrakproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://ecc:***@localhost:5432/amtrakproject\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1347202</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1347202,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*)\n",
    "FROM weather_hourly;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
