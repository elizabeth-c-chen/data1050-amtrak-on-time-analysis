{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b64903",
   "metadata": {},
   "source": [
    "# ETL and EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bd0e4",
   "metadata": {},
   "source": [
    "# Part 1 - Amtrak Northeast Regional Train Data\n",
    "* This project would not be possible without the diligent joint effort by [Chris Juckins](https://juckins.net/index.php) and [John Bobinyec](http://dixielandsoftware.net/Amtrak/status/StatusMaps/) to collect and preserve Amtrak's on-time performance records. Chris Juckins' archive of timetables was another invaluable resource which enabled me to sort through the trains and stations I chose to use in this project.\n",
    "* The train data is sourced from [Amtrak Status Maps Archive Database (ASMAD)](https://juckins.net/amtrak_status/archive/html/home.php), and has been retrieved with Chris' permission.\n",
    "\n",
    "### Overview of the Process\n",
    "* Functions were written to scrape the HTML table returned from the search query and to process each column to the desired format\n",
    "* Additional columns were also added during processing to aid in joining the train data with weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770400b6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from trains_retrieve_and_process_data import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bee1b4",
   "metadata": {},
   "source": [
    "### Retrieve HTML table data and recreate as a Pandas DataFrame\n",
    "* Default is to collect data from the previous day (run after 5am or else no data will be retrieved, ASMAD updates around 4am)\n",
    "* Collects both arrival and departure data and stores in a dictionary further indexed by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2021,4,25)\n",
    "end = date(2021,6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = retrieve_data(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd909dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "depart =  raw_data_to_raw_df(raw_data, 'Depart')\n",
    "print(depart.shape[0])\n",
    "depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = raw_data_to_raw_df(raw_data, 'Arrive')\n",
    "print(arrive.shape[0])\n",
    "arrive.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35503c7",
   "metadata": {},
   "source": [
    "### Save the raw DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring = './data/trains_raw/arrive_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "depart_filestring = './data/trains_raw/depart_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "\n",
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd1957",
   "metadata": {},
   "source": [
    "### Process the raw DF with modifications/additions \n",
    "* Modifications to the data:\n",
    "    * Separate the Origin Date and Origin Week Day  into two columns\n",
    "    * Add separate columns for Origin Year and Origin Month\n",
    "    * Separate the Scheduled Arrival/Departure Date, Scheduled Arrival/Departure Week Day, and Scheduled Arrival/Departure Time into three seperate columns\n",
    "    * Calculate the value of the time difference between Scheduled and Actual Arrival/Departure\n",
    "    * Convert Service Disruption and Cancellation column text flags to binary indicator columns\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a9f09",
   "metadata": {},
   "source": [
    "### For new 2021 data, concatenate with previously retrieved and processed data from this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b52e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring2021 = './data/trains/arrive_2021_processed.csv'\n",
    "depart_filestring2021 = './data/trains/depart_2021_processed.csv'\n",
    "        \n",
    "prev_arrive2021 = pd.read_csv(arrive_filestring2021)\n",
    "prev_depart2021 = pd.read_csv(depart_filestring2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021 = pd.concat([prev_arrive2021, full_arrive], ignore_index=True, axis=0)\n",
    "new_depart2021 = pd.concat([prev_depart2021, full_depart], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8436b",
   "metadata": {},
   "source": [
    "### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95aee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a231e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.to_csv(arrive_filestring2021, line_terminator='\\n', index=False)\n",
    "new_depart2021.to_csv(depart_filestring2021, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdecb55",
   "metadata": {},
   "source": [
    "# Part 2 - Visual Crossing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe05c71",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f27186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a70afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_retrieve_and_process_data import *\n",
    "assert os.environ.get('VC_TOKEN') is not None , 'empty token!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068b6ac",
   "metadata": {},
   "source": [
    "### Retrieve unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3831a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str(date(2021,4,25))\n",
    "end = str(date.today()-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc081561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for LOCATION: Boston_MA\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Providence_RI\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Kingston_RI\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Westerly_RI\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Mystic_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: New_London_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Old_Saybrook_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: New_Haven_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Bridgeport_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Stamford_CT\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: New_Rochelle_NY\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Manhattan_NY\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Newark_NJ\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Iselin_NJ\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Trenton_NJ\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Philadelphia_PA\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Wilmington_DE\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Aberdeen_MD\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Baltimore_MD\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Baltimore_BWI_Airport_MD\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: New_Carrollton_MD\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Retrieving data for LOCATION: Washington_DC\n",
      "    and DATE RANGE: 2021-04-25T00:00:00 to 2021-06-01T23:59:00\n",
      "Successfully collected data has been saved at the following filenames:\n",
      "        FILE:   ./data/weather_raw/Boston_MA_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Providence_RI_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Kingston_RI_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Westerly_RI_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Mystic_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/New_London_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Old_Saybrook_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/New_Haven_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Bridgeport_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Stamford_CT_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/New_Rochelle_NY_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Manhattan_NY_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Newark_NJ_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Iselin_NJ_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Trenton_NJ_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Philadelphia_PA_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Wilmington_DE_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Aberdeen_MD_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Baltimore_MD_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Baltimore_BWI_Airport_MD_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/New_Carrollton_MD_weather_data_2021-04-25_2021-06-01.csv\n",
      "        FILE:   ./data/weather_raw/Washington_DC_weather_data_2021-04-25_2021-06-01.csv\n"
     ]
    }
   ],
   "source": [
    "successful_retrievals = retrieve_weather_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36c337",
   "metadata": {},
   "source": [
    "### Data Cleaning/Taking Subset of Columns\n",
    "\n",
    "* Processing recent data by year - add new columns, make minor fixes to string format, take subset of full columns list.\n",
    "* Function processes the files that were successfully created in the previous step.\n",
    "* This part is assuming 2021 data is being read and concatenates the previously retrieved data with the new data to create a single combined file.\n",
    "* Output shows the fraction of the data kept, data is valid and complete almost always ($> 99\\%$ of original data has been retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19df3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and combined the following raw data files with previous data:\n",
      "        FILE:          ./data/weather/Boston_MA_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.18311403508771928\n",
      "        FILE:          ./data/weather/Providence_RI_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.23135964912280702\n",
      "        FILE:          ./data/weather/Kingston_RI_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.18859649122807018\n",
      "        FILE:          ./data/weather/Westerly_RI_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.15460526315789475\n",
      "        FILE:          ./data/weather/Mystic_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.15460526315789475\n",
      "        FILE:          ./data/weather/New_London_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.15460526315789475\n",
      "        FILE:          ./data/weather/Old_Saybrook_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.20614035087719298\n",
      "        FILE:          ./data/weather/New_Haven_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.17653508771929824\n",
      "        FILE:          ./data/weather/Bridgeport_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.1699561403508772\n",
      "        FILE:          ./data/weather/Stamford_CT_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.1787280701754386\n",
      "        FILE:          ./data/weather/New_Rochelle_NY_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.16885964912280702\n",
      "        FILE:          ./data/weather/Manhattan_NY_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.13596491228070176\n",
      "        FILE:          ./data/weather/Newark_NJ_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.16776315789473684\n",
      "        FILE:          ./data/weather/Iselin_NJ_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.17543859649122806\n",
      "        FILE:          ./data/weather/Trenton_NJ_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.14583333333333334\n",
      "        FILE:          ./data/weather/Philadelphia_PA_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.14912280701754385\n",
      "        FILE:          ./data/weather/Wilmington_DE_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.1513157894736842\n",
      "        FILE:          ./data/weather/Aberdeen_MD_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.13157894736842105\n",
      "        FILE:          ./data/weather/Baltimore_MD_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.13048245614035087\n",
      "        FILE:          ./data/weather/Baltimore_BWI_Airport_MD_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.13048245614035087\n",
      "        FILE:          ./data/weather/New_Carrollton_MD_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.13267543859649122\n",
      "        FILE:          ./data/weather/Washington_DC_weather_subset_2021.csv\n",
      "        FRACTION KEPT: 0.12609649122807018\n"
     ]
    }
   ],
   "source": [
    "process_weather_data(files_to_process=successful_retrievals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f95a2c",
   "metadata": {},
   "source": [
    "### Data sample for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/weather/Providence_RI_weather_2021_subset.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8693d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9be4bd",
   "metadata": {},
   "source": [
    "# Part 3a: Loading Data into Postgres Database (Composite Primary Key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca8a15",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "import time\n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8e2da",
   "metadata": {},
   "source": [
    "#### Functions to create and update tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, command):\n",
    "    \"\"\"\n",
    "    Create a table in the PostgreSQL database from the specified command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "def update_table(conn, command, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from a CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "        \n",
    "def update_trains(conn, command, arr_or_dep, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from trains CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple([arr_or_dep] + row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99334dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_station_info_table_command = \"\"\" \n",
    "                                    DROP TABLE IF EXISTS station_info CASCADE;\n",
    "\n",
    "                                    CREATE TABLE station_info (\n",
    "                                        station_code text PRIMARY KEY,\n",
    "                                        station_name text,\n",
    "                                        state text,\n",
    "                                        amtrak_city text,\n",
    "                                        weather_loc text,\n",
    "                                        longitude real,\n",
    "                                        latitude real,\n",
    "                                        nb_mile numeric,\n",
    "                                        sb_mile numeric\n",
    "                                    );\n",
    "                                    \"\"\"\n",
    "\n",
    "insert_into_station_info_table_command = \"\"\"\n",
    "                                         INSERT INTO station_info (\n",
    "                                             station_code,\n",
    "                                             station_name,\n",
    "                                             state,\n",
    "                                             amtrak_city,\n",
    "                                             weather_loc,\n",
    "                                             longitude,\n",
    "                                             latitude,\n",
    "                                             nb_mile,\n",
    "                                             sb_mile\n",
    "                                        )\n",
    "                                        VALUES\n",
    "                                            (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                        ON CONFLICT DO NOTHING;\n",
    "                                        \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trains_table_command = \"\"\" \n",
    "                              DROP TABLE IF EXISTS all_trains CASCADE;\n",
    "                              CREATE TABLE all_trains (\n",
    "                                  arr_or_dep text,\n",
    "                                  train_num text,\n",
    "                                  station_code text REFERENCES station_info (station_code), \n",
    "                                  direction text,\n",
    "                                  origin_date date,\n",
    "                                  origin_year int,\n",
    "                                  origin_month int,\n",
    "                                  origin_week_day text,\n",
    "                                  full_sched_arr_dep_datetime timestamp,\n",
    "                                  sched_arr_dep_date date,\n",
    "                                  sched_arr_dep_week_day text,\n",
    "                                  sched_arr_dep_time time,\n",
    "                                  act_arr_dep_time time,\n",
    "                                  full_act_arr_dep_datetime timestamp,\n",
    "                                  timedelta_from_sched numeric,\n",
    "                                  service_disruption boolean,\n",
    "                                  cancellations boolean,\n",
    "                                  PRIMARY KEY (train_num, station_code, origin_date)\n",
    "                              );\n",
    "                              \"\"\"\n",
    "\n",
    "insert_into_trains_table_command = \"\"\"\n",
    "                                     INSERT INTO all_trains (\n",
    "                                          arr_or_dep,\n",
    "                                          train_num,\n",
    "                                          station_code,\n",
    "                                          direction,\n",
    "                                          origin_date,\n",
    "                                          origin_year,\n",
    "                                          origin_month,\n",
    "                                          origin_week_day,\n",
    "                                          full_sched_arr_dep_datetime,\n",
    "                                          sched_arr_dep_date,\n",
    "                                          sched_arr_dep_week_day,\n",
    "                                          sched_arr_dep_time,\n",
    "                                          act_arr_dep_time,\n",
    "                                          full_act_arr_dep_datetime,\n",
    "                                          timedelta_from_sched,\n",
    "                                          service_disruption,\n",
    "                                          cancellations\n",
    "                                     )\n",
    "                                     VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                     ON CONFLICT DO NOTHING; \n",
    "                                     \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_table_command = \"\"\"\n",
    "                               DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "                               CREATE TABLE weather_hourly (\n",
    "                                   location text,\n",
    "                                   date_time timestamp,\n",
    "                                   temperature real,\n",
    "                                   precipitation real,\n",
    "                                   cloud_cover real,\n",
    "                                   conditions text, \n",
    "                                   weather_type text,\n",
    "                                   latitude real,\n",
    "                                   longitude real,\n",
    "                                   PRIMARY KEY (date_time, location)\n",
    "                               );\n",
    "                               \"\"\"\n",
    "\n",
    "insert_into_weather_table_command = \"\"\"\n",
    "                                    INSERT INTO weather_hourly (\n",
    "                                        location,\n",
    "                                        date_time,\n",
    "                                        temperature,\n",
    "                                        precipitation,\n",
    "                                        cloud_cover,\n",
    "                                        conditions,\n",
    "                                        weather_type,\n",
    "                                        latitude,\n",
    "                                        longitude\n",
    "                                    )\n",
    "                                    VALUES\n",
    "                                        (%s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                                    ON CONFLICT DO NOTHING;\n",
    "                                    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f346bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_route_table_command = \"\"\"\n",
    "                             DROP TABLE IF EXISTS regional_route CASCADE;\n",
    "                            \n",
    "                             CREATE TABLE regional_route (\n",
    "                                 coord_id SERIAL PRIMARY KEY,\n",
    "                                 longitude real,\n",
    "                                 latitude real,\n",
    "                                 path_group numeric,\n",
    "                                 connecting_path text, \n",
    "                                 nb_station_group text,\n",
    "                                 sb_station_group text\n",
    "                             );\n",
    "                             \"\"\"\n",
    "\n",
    "insert_into_route_table_command = \"\"\"\n",
    "                                  INSERT INTO regional_route (\n",
    "                                      longitude,\n",
    "                                      latitude, \n",
    "                                      path_group,\n",
    "                                      connecting_path,\n",
    "                                      nb_station_group,\n",
    "                                      sb_station_group\n",
    "                                  )\n",
    "                                  VALUES \n",
    "                                      (%s, %s, %s, %s, %s, %s) \n",
    "                                  ON CONFLICT DO NOTHING;\n",
    "                                  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ac833",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_commands = [create_station_info_table_command,\n",
    "                         create_trains_table_command,\n",
    "                         create_weather_table_command,\n",
    "                         create_route_table_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='amtrakproject' user='{}' password={}\".format(os.environ.get('USER'), os.environ.get('DB_PASS')))\n",
    "assert conn is not None, 'need to fix conn!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "for command in create_table_commands:\n",
    "    create_table(conn, command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all station facts into station info table\n",
    "update_table(conn, insert_into_station_info_table_command, './data/facts/geo_stations_info.csv')\n",
    "\n",
    "# Insert route with the coordiniates into route table\n",
    "update_table(conn, insert_into_route_table_command, './data/facts/NE_regional_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdde067",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_trains_table_command)\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "begin_everything = time.time()\n",
    "\n",
    "# Insert all train data into arrival and departure data tables\n",
    "for year in years:\n",
    "    start = time.time()\n",
    "    arrive_csv = './data/trains/arrive_{}_processed.csv'.format(year)\n",
    "    depart_csv = './data/trains/depart_{}_processed.csv'.format(year)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Arrival', arrive_csv)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Departure', depart_csv)\n",
    "    print('DONE WITH', year, 'in', time.time() - start)\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1facf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_weather_table_command)\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'Westerly_RI', 'Mystic_CT',\n",
    "                            'New_London_CT', 'Old_Saybrook_CT', 'New_Haven_CT', 'Bridgeport_CT', \n",
    "                            'Stamford_CT', 'New_Rochelle_NY', 'Manhattan_NY', 'Newark_NJ', 'Iselin_NJ', \n",
    "                            'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE','Aberdeen_MD', 'Baltimore_MD',\n",
    "                            'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all weather data into the weather data table\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        weather_csv = './data/weather/{}_weather_subset_{}.csv'.format(location, year)\n",
    "        update_table(conn, insert_into_weather_table_command, weather_csv)\n",
    "    print('Finished adding location', location, 'to the database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328db4e",
   "metadata": {},
   "source": [
    "# Part 3b: Loading Data into Postgres Database (Serial Primary Key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf687fe",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "import time\n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, command):\n",
    "    \"\"\"\n",
    "    Create a table in the PostgreSQL database from the specified command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "def update_table(conn, command, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from a CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "        \n",
    "def update_trains(conn, command, arr_or_dep, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from trains CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple([arr_or_dep] + row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930aaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_station_info_table = \"\"\" \n",
    "                            DROP TABLE IF EXISTS station_info CASCADE;\n",
    "\n",
    "                            CREATE TABLE station_info (\n",
    "                                station_id SERIAL PRIMARY KEY,\n",
    "                                station_code text,\n",
    "                                amtrak_station text,\n",
    "                                crew_change boolean,\n",
    "                                weather_station text,\n",
    "                                longitude real,\n",
    "                                latitude real,\n",
    "                                nb_mile numeric,\n",
    "                                sb_mile numeric\n",
    "                            );\n",
    "                            \"\"\"\n",
    "\n",
    "insert_into_station_info_table = \"\"\"\n",
    "                                 INSERT INTO station_info (\n",
    "                                     station_code,\n",
    "                                     amtrak_station,\n",
    "                                     crew_change,\n",
    "                                     weather_station,\n",
    "                                     longitude,\n",
    "                                     latitude,\n",
    "                                     nb_mile,\n",
    "                                     sb_mile\n",
    "                                 )\n",
    "                                 VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                 ON CONFLICT DO NOTHING;\n",
    "                                 \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trains_table = \"\"\" \n",
    "                      DROP TABLE IF EXISTS all_trains CASCADE;\n",
    "                      \n",
    "                      CREATE TABLE all_trains (\n",
    "                          dataset_id SERIAL PRIMARY KEY,\n",
    "                          arr_or_dep text,\n",
    "                          train_num text,\n",
    "                          station_code text, \n",
    "                          direction text,\n",
    "                          origin_date date,\n",
    "                          origin_year int,\n",
    "                          origin_month int,\n",
    "                          origin_week_day text,\n",
    "                          full_sched_arr_dep_datetime timestamp,\n",
    "                          sched_arr_dep_date date,\n",
    "                          sched_arr_dep_week_day text,\n",
    "                          sched_arr_dep_time time,\n",
    "                          act_arr_dep_time time,\n",
    "                          full_act_arr_dep_datetime timestamp,\n",
    "                          timedelta_from_sched numeric,\n",
    "                          service_disruption boolean,\n",
    "                          cancellations boolean\n",
    "                      );\n",
    "                      \"\"\"\n",
    "\n",
    "insert_into_trains_table = \"\"\"\n",
    "                           INSERT INTO all_trains (\n",
    "                               arr_or_dep,\n",
    "                               train_num,\n",
    "                               station_code,\n",
    "                               direction,\n",
    "                               origin_date,\n",
    "                               origin_year,\n",
    "                               origin_month,\n",
    "                               origin_week_day,\n",
    "                               full_sched_arr_dep_datetime,\n",
    "                               sched_arr_dep_date,\n",
    "                               sched_arr_dep_week_day,\n",
    "                               sched_arr_dep_time,\n",
    "                               act_arr_dep_time,\n",
    "                               full_act_arr_dep_datetime,\n",
    "                               timedelta_from_sched,\n",
    "                               service_disruption,\n",
    "                               cancellations\n",
    "                          )\n",
    "                          VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                          ON CONFLICT DO NOTHING; \n",
    "                          \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2eb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_table = \"\"\"\n",
    "                       DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "                       CREATE TABLE weather_hourly (\n",
    "                           weather_id SERIAL PRIMARY KEY,\n",
    "                           location text,\n",
    "                           date_time timestamp,\n",
    "                           temperature real,\n",
    "                           precipitation real,\n",
    "                           cloud_cover real,\n",
    "                           conditions text, \n",
    "                           weather_type text,\n",
    "                           latitude real,\n",
    "                           longitude real\n",
    "                       );\n",
    "                       \"\"\"\n",
    "\n",
    "insert_into_weather_table = \"\"\"\n",
    "                            INSERT INTO weather_hourly (\n",
    "                                location,\n",
    "                                date_time,\n",
    "                                temperature,\n",
    "                                precipitation,\n",
    "                                cloud_cover,\n",
    "                                conditions,\n",
    "                                weather_type,\n",
    "                                latitude,\n",
    "                                longitude\n",
    "                            )\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                            ON CONFLICT DO NOTHING;\n",
    "                            \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_route_table = \"\"\"\n",
    "                     DROP TABLE IF EXISTS regional_route CASCADE;\n",
    "\n",
    "                     CREATE TABLE regional_route (\n",
    "                         coord_id SERIAL PRIMARY KEY,\n",
    "                         longitude real,\n",
    "                         latitude real,\n",
    "                         path_group int,\n",
    "                         station_pairing text, \n",
    "                         nb_station_group text,\n",
    "                         sb_station_group text\n",
    "                     );\n",
    "                     \"\"\"\n",
    "\n",
    "insert_into_route_table = \"\"\"\n",
    "                          INSERT INTO\n",
    "                              regional_route (\n",
    "                                  longitude,\n",
    "                                  latitude, \n",
    "                                  path_group,\n",
    "                                  station_pairing,\n",
    "                                  nb_station_group,\n",
    "                                  sb_station_group\n",
    "                              )\n",
    "                          VALUES \n",
    "                              (%s, %s, %s, %s, %s, %s) \n",
    "                          ON CONFLICT DO NOTHING;\n",
    "                          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ad026",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='amtrakproject' user='{}' password={}\".format(os.environ.get('USER'), os.environ.get('DB_PASS')))\n",
    "assert conn is not None, 'need to fix conn!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create station link table\n",
    "create_table(conn, create_station_info_table)\n",
    "\n",
    "# Create route coordinates table\n",
    "create_table(conn, create_route_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all station facts into station info table\n",
    "update_table(conn, insert_into_station_info_table, './data/facts/geo_stations_info.csv')\n",
    "\n",
    "# Insert route with the coordinates into route table\n",
    "update_table(conn, insert_into_route_table, './data/facts/NE_regional_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_trains_table)\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all train data into arrival and departure data tables\n",
    "begin_everything = time.time()\n",
    "for year in years:\n",
    "    start = time.time()\n",
    "    arrive_csv = './data/trains/arrive_{}_processed.csv'.format(year)\n",
    "    depart_csv = './data/trains/depart_{}_processed.csv'.format(year)\n",
    "    update_trains(conn, insert_into_trains_table, 'Arrival', arrive_csv)\n",
    "    update_trains(conn, insert_into_trains_table, 'Departure', depart_csv)\n",
    "    print('Finished adding year', year, 'to database in', time.time() - start, 'seconds')\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_weather_table)\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'Westerly_RI', 'Mystic_CT',\n",
    "                            'New_London_CT', 'Old_Saybrook_CT', 'New_Haven_CT', 'Bridgeport_CT', \n",
    "                            'Stamford_CT', 'New_Rochelle_NY', 'Manhattan_NY', 'Newark_NJ', 'Iselin_NJ', \n",
    "                            'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE','Aberdeen_MD', 'Baltimore_MD',\n",
    "                            'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all weather data into the weather data table\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        weather_csv = './data/weather/{}_weather_subset_{}.csv'.format(location, year)\n",
    "        update_table(conn, insert_into_weather_table, weather_csv)\n",
    "    print('Finished adding location', location, 'to database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
