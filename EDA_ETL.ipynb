{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and ETL Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "from fetch_data import construct_urls, fetch_data_from_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Helper Functions for loading/requesting and processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(option='from_saved', start=date.today()-timedelta(days=1), end=date.today()):\n",
    "    \"\"\"\n",
    "    Function to either load raw data from previously saved CSV files, or retrieve it again\n",
    "    from the website.\n",
    "    \"\"\"\n",
    "    if option == 'request':\n",
    "        northbound = [[66, 82, 86, 88, 94], [132, 150, 160, 162, 164, 166], [168, 170, 172, 174]]\n",
    "        southbound = [[67, 83, 93, 95, 99], [135, 137, 139, 161, 163, 165], [167, 171, 173, 175, 195]]\n",
    "        urls = construct_urls(northbound, southbound, start, end)\n",
    "        data = fetch_data_from_urls(urls)\n",
    "    elif option == 'from_saved':\n",
    "        data = None\n",
    "        print(\"Skip this section and go to part B!\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an option\n",
    "* If `option = 'from_saved'`, go to section on Raw Data.\n",
    "* Otherwise, uncomment the other line and wait for request to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = pull_data(option='from_saved')\n",
    "start = date(2020,11,29)\n",
    "end = date(2021,2,18)\n",
    "data = pull_data(option='request', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(num):\n",
    "    \"\"\"\n",
    "    Return direction of the train (odd = Southbound, even = Northbound).\n",
    "    \"\"\"\n",
    "    if num % 2 == 0:\n",
    "        return 'Northbound'\n",
    "    else:\n",
    "        return 'Southbound'\n",
    "\n",
    "\n",
    "def get_num(re_match):\n",
    "    \"\"\"\n",
    "    Assuming input contains a match , extract and return the numerical data from input.\n",
    "    \"\"\"\n",
    "    num_match = re.search('(?P<num>[0-9]+)', re_match)\n",
    "    return int(num_match.group('num'))\n",
    "\n",
    "\n",
    "def make_dict_from_cols(col_names):\n",
    "    \"\"\"\n",
    "    Create dictionary from a list of column names\n",
    "    \"\"\"\n",
    "    dictionary = { col_name: [] for col_name in col_names }\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_html_col_names(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Using NYP (station with both arrival times and departure times), \n",
    "    retrieve column names from the HTML table, located in the 2nd row.\n",
    "    \"\"\"\n",
    "    data_list = raw_data[arrive_or_depart]['BOS']\n",
    "    page_content = data_list[0]\n",
    "    doc = lh.fromstring(page_content)\n",
    "    tr_elements = doc.xpath('//tr')\n",
    "    html_col_names = [entry.text_content().strip() for entry in tr_elements[1]]        \n",
    "    return html_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_to_raw_df(raw_data, arrive_or_depart):\n",
    "    \"\"\"\n",
    "    Function to put the raw html data in a dataframe for ease of processing.\n",
    "    \"\"\"\n",
    "    col_names = get_html_col_names(raw_data, arrive_or_depart)\n",
    "    N = 7\n",
    "    data_dict = make_dict_from_cols(['Direction', 'Station'] + col_names)\n",
    "    for station in raw_data[arrive_or_depart].keys():\n",
    "        data_list = raw_data[arrive_or_depart][station]\n",
    "        L = len(data_list)\n",
    "        for i in range(L):\n",
    "            page_content = data_list[i]\n",
    "            doc = lh.fromstring(page_content)\n",
    "            tr_elements = doc.xpath('//tr')\n",
    "            if len(tr_elements) > 3:\n",
    "                title = tr_elements[0].text_content()\n",
    "                direction = get_direction(get_num(title))\n",
    "                for j in range(2, len(tr_elements)):\n",
    "                    table_row = tr_elements[j] \n",
    "                    if len(table_row) == N:\n",
    "                        data_dict['Direction'].append(direction)\n",
    "                        data_dict['Station'].append(station)\n",
    "                        for col_name, entry in zip(col_names, table_row):\n",
    "                            data = entry.text_content()\n",
    "                            data_dict[col_name].append(data)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "            else:\n",
    "                print(\"Potentially no data for this time period, or an error occurred\", station, arrive_or_depart)\n",
    "    return pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "depart =  raw_data_to_raw_df(data, 'Depart')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive = data['Arrive']\n",
    "start_time = time.time()\n",
    "arrive = raw_data_to_raw_df(data, 'Arrive')\n",
    "print('elapsed:', time.time() - start_time)\n",
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2020,11,29)\n",
    "end = date(2021,2,18)\n",
    "arrive_filestring = './data/trains/raw_arrive_' + str(start) + '_' + str(end) + '.csv'\n",
    "depart_filestring = './data/trains/raw_depart_' + str(start) + '_' +  str(end) + '.csv'\n",
    "print(arrive_filestring)\n",
    "print(depart_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Train Data - Scraped and Loaded into Pandas DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scraped from an HTML table, so the raw data doesn't look nice after scraping until it's put back in a dataframe. The data was then processed into an initial dataframe and saved as a CSV for later processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = pd.read_csv(arrive_filestring, lineterminator='\\n', keep_default_na=False)\n",
    "depart = pd.read_csv(depart_filestring, lineterminator='\\n', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return ['Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Ar Date', 'Sch Ar Date', \n",
    "                 'Sch Ar Day', 'Sch Ar Time','Act Ar Time', 'Arrive Diff', 'Service Disruption', 'Cancellations']\n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return [ 'Train Num',  'Station', 'Direction', 'Origin Date', 'Origin Year', 'Origin Quarter', \n",
    "                 'Origin Month', 'Origin Day', 'Origin Week Day', 'Full Sch Dp Date','Sch Dp Date', \n",
    "                 'Sch Dp Day', 'Sch Dp Time','Act Dp Time', 'Depart Diff', 'Service Disruption', 'Cancellations']\n",
    "\n",
    "    \n",
    "def get_key_names(arrive_or_depart):\n",
    "    if arrive_or_depart == 'Arrive':\n",
    "        return {'Sch Full Date': 'Full Sch Ar Date', 'Sch Abbr': 'Sch Ar', 'Act Abbr': 'Act Ar', 'Diff': 'Arrive Diff'}\n",
    "    \n",
    "    elif arrive_or_depart == 'Depart':\n",
    "        return {'Sch Full Date': 'Full Sch Dp Date', 'Sch Abbr': 'Sch Dp', 'Act Abbr': 'Act Dp', 'Diff': 'Depart Diff'}\n",
    "\n",
    "\n",
    "def process_columns(df, arrive_or_depart):\n",
    "    new_cols = get_col_names(arrive_or_depart)\n",
    "    ad_keys = get_key_names(arrive_or_depart) # the specific keys depending on if new_df is for arr or dep data\n",
    "    \n",
    "    new_df = pd.DataFrame(columns=new_cols)\n",
    "    new_df['Train Num'] = pd.to_numeric(df['Train #'])\n",
    "    new_df['Station'] = df['Station']\n",
    "    new_df['Direction'] = df['Direction']\n",
    "    \n",
    "    origin_date = pd.to_datetime(df['Origin Date'], format=\"%m/%d/%Y\", exact=False, errors='coerce')    \n",
    "    new_df['Origin Date'] = origin_date\n",
    "    new_df['Origin Year'] = origin_date.dt.year\n",
    "    new_df['Origin Quarter'] = origin_date.dt.quarter\n",
    "    new_df['Origin Month'] = origin_date.dt.month\n",
    "    new_df['Origin Day'] = origin_date.dt.day\n",
    "    new_df['Origin Week Day'] = origin_date.dt.day_name()\n",
    "    \n",
    "    sched_full_date = pd.to_datetime(df[ad_keys['Sch Abbr']], format='%m/%d/%Y %I:%M %p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Sch Full Date']] = sched_full_date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Date'] = sched_full_date.dt.date\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Day'] = sched_full_date.dt.day_name()\n",
    "    new_df[ad_keys['Sch Abbr'] + ' Time'] = sched_full_date.dt.time\n",
    "    act_time = pd.to_datetime(df[ad_keys['Act Abbr']], format='%I:%M%p', exact=False, errors='coerce')\n",
    "    new_df[ad_keys['Act Abbr'] + ' Time'] = act_time.dt.time\n",
    "    \n",
    "    df['Sched Date'] = sched_full_date \n",
    "    df['Act Date'] = pd.to_datetime(sched_full_date.dt.date.astype(str) + \" \" + df[ad_keys['Act Abbr']].astype(str),exact=False, errors='coerce')\n",
    "    max_expected_delay = pd.Timedelta(hours=10)\n",
    "    delta = df['Act Date'] - df['Sched Date']\n",
    "    m_late = (delta < max_expected_delay) & (-1*max_expected_delay > delta)\n",
    "    m_early = (-1*delta < max_expected_delay) & (-1*max_expected_delay > -1*delta)\n",
    "    df.loc[m_late, 'Act Date'] += pd.Timedelta(days=1)\n",
    "    df.loc[m_early, 'Act Date'] -= pd.Timedelta(days=1)\n",
    "    new_df[ad_keys['Diff']] = np.rint((df['Act Date'] - df['Sched Date']).dt.total_seconds()/60).astype(int)\n",
    "    new_df['Service Disruption'] = df['Service Disruption'].replace('SD', 1).replace('', 0)\n",
    "    new_df['Cancellations'] =  df['Cancellations'].replace('C', 1).replace('', 0)\n",
    "    return new_df.replace('', np.nan).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_arrive.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the number of rows in each df that are omitted in the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_arrive.loc[(full_arrive['Origin Year'] == 2010)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_depart.loc[(full_depart['Origin Year'] == 2010)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files by year to break down data into smaller chunks\n",
    "* Ignore any data from 2010, this is only 23 rows in the departure and arrival dataframes combined (due to trains that were retrieved with the web request starting 1/1/2011 but originated in 2010). \n",
    "* Subset into files by arrival and departure by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "#for year in years:\n",
    "#    depart_subset = full_depart.loc[(full_depart['Origin Year'] == year)]\n",
    "#    arrive_subset = full_arrive.loc[(full_arrive['Origin Year'] == year)]\n",
    "#    print(depart_subset.shape[0], arrive_subset.shape[0])\n",
    "#    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "#    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "#    depart_subset.to_csv(depart_filestring, line_terminator='\\n', index=False)\n",
    "#    arrive_subset.to_csv(arrive_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev_arrive_2021 = pd.read_csv('./data/trains/processed_arrive_2021.csv')\n",
    "#prev_depart_2021 = pd.read_csv('./data/trains/processed_depart_2021.csv')\n",
    "\n",
    "#new_arrive_2021 = pd.concat([prev_arrive_2021, full_arrive], ignore_index=True, axis=0)\n",
    "#new_depart_2021 = pd.concat([prev_depart_2021, full_depart], ignore_index=True, axis=0)\n",
    "\n",
    "#new_arrive_2021.to_csv('./data/trains/processed_arrive_2021.csv', line_terminator='\\n', index=False)\n",
    "#new_depart_2021.to_csv('./data/trains/processed_depart_2021.csv', line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Postgres Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "\n",
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS train_info CASCADE;\n",
    "        CREATE TABLE train_info (\n",
    "            train_info_id SERIAL PRIMARY KEY,\n",
    "            train_num int UNIQUE,\n",
    "            operating_direction text,\n",
    "            reg_operates_on_mon boolean,\n",
    "            reg_operates_on_tues boolean,\n",
    "            reg_operates_on_wed boolean,\n",
    "            reg_operates_on_thurs boolean,\n",
    "            reg_operates_on_fri boolean,\n",
    "            reg_operates_on_sat boolean,\n",
    "            reg_operates_on_sun boolean,\n",
    "            depart_origin_time text,\n",
    "            depart_NY_time text,\n",
    "            arrive_dest_time text\n",
    "            \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS arrivals CASCADE;\n",
    "        CREATE TABLE arrivals (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num int REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_arr_datetime timestamp,\n",
    "            sched_arr_date date,\n",
    "            sched_arr_week_day text,\n",
    "            sched_arr_time time,\n",
    "            act_arr_time time,\n",
    "            arrive_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\" \n",
    "        DROP TABLE IF EXISTS departures CASCADE;\n",
    "        CREATE TABLE departures (\n",
    "            dataset_id SERIAL PRIMARY KEY,\n",
    "            train_num int REFERENCES train_info (train_num),\n",
    "            station_code text, \n",
    "            direction text,\n",
    "            origin_date date,\n",
    "            origin_year int,\n",
    "            origin_quarter int,\n",
    "            origin_month int,\n",
    "            origin_day int,\n",
    "            origin_week_day text,\n",
    "            full_sched_dep_datetime timestamp,\n",
    "            sched_dep_date date,\n",
    "            sched_dep_week_day text,\n",
    "            sched_dep_time time,\n",
    "            act_dep_time time,\n",
    "            depart_diff numeric,\n",
    "            service_disruption boolean,\n",
    "            cancellations boolean     \n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql \n",
    "\n",
    "def update_train_info_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO train_info (train_num, operating_direction, reg_operates_on_mon, \n",
    "                   reg_operates_on_tues, reg_operates_on_wed, reg_operates_on_thurs, \n",
    "                   reg_operates_on_fri, reg_operates_on_sat, reg_operates_on_sun, \n",
    "                   depart_origin_time, depart_NY_time, arrive_dest_time)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                \n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "\n",
    "def update_arrive_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO arrivals (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_arr_datetime, sched_arr_date, sched_arr_week_day,\n",
    "                               sched_arr_time, act_arr_time, arrive_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)     # skip header                                                                         \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n",
    "\n",
    "def update_depart_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO departures (train_num, station_code, direction, origin_date, origin_year, origin_quarter, origin_month, \n",
    "                               origin_day, origin_week_day, full_sched_dep_datetime, sched_dep_date, sched_dep_week_day,\n",
    "                               sched_dep_time, act_dep_time, depart_diff, service_disruption, cancellations) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\"\"\"]                        \n",
    "    with open(csv_file, newline='') as file: \n",
    "        train_reader = csv.reader(file, delimiter=',')\n",
    "        next(train_reader, None)   # skip header                                                                           \n",
    "        for row in train_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "depart_filestrings_list = []\n",
    "arrive_filestrings_list = []\n",
    "for year in years:\n",
    "    depart_filestring = './data/trains/processed_depart_' + str(year) + '.csv'\n",
    "    arrive_filestring = './data/trains/processed_arrive_' + str(year) + '.csv'\n",
    "    depart_filestrings_list.append(depart_filestring) \n",
    "    arrive_filestrings_list.append(arrive_filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)\n",
    "begin_everything = time.time()\n",
    "update_train_info_table(conn, './data/trains/train_nums.csv')\n",
    "for i in range(len(years)):\n",
    "    start = time.time()\n",
    "    arrive_csv = arrive_filestrings_list[i]\n",
    "    depart_csv = depart_filestrings_list[i]\n",
    "    update_arrive_table(conn, arrive_csv)\n",
    "    update_depart_table(conn, depart_csv)\n",
    "    print(\"DONE WITH\", years[i], 'in', time.time() - start)\n",
    "conn.close()\n",
    "\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys \n",
    "from sqlalchemy import text\n",
    "from psycopg2 import sql\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = [('2011-01-01','2011-12-31'), # Original dates list used to get all historical data\n",
    "              ('2012-01-01','2012-12-31'),\n",
    "              ('2013-01-01','2013-12-31'),\n",
    "              ('2014-01-01','2014-12-31'),\n",
    "              ('2015-01-01','2015-12-31'),\n",
    "              ('2016-01-01','2016-12-31'),\n",
    "              ('2017-01-01','2017-12-31'),\n",
    "              ('2018-01-01','2018-12-31'),\n",
    "              ('2019-01-01','2019-12-31'),\n",
    "              ('2020-01-01','2020-11-31')]\n",
    "\n",
    "dates_list = [('2011-01-01','2011-12-31'), # New dates list needed for DB\n",
    "              ('2012-01-01','2012-12-31'),\n",
    "              ('2013-01-01','2013-12-31'),\n",
    "              ('2014-01-01','2014-12-31'),\n",
    "              ('2015-01-01','2015-12-31'),\n",
    "              ('2016-01-01','2016-12-31'),\n",
    "              ('2017-01-01','2017-12-31'),\n",
    "              ('2018-01-01','2018-12-31'),\n",
    "              ('2019-01-01','2019-12-31'),\n",
    "              ('2020-01-01','2021-02-18')]\n",
    "\n",
    "locations = ['Boston,MA', 'Providence,RI', 'Kingston,RI', 'New%20London,CT', 'New%20Haven,CT', 'Stamford,CT', \n",
    "             'Manhattan,NY', 'Newark,NJ', 'Trenton,NJ', 'Philadelphia,PA', 'Wilmington,DE', 'Baltimore,MD', \n",
    "             'Baltimore%20BWI%20Airport,MD', 'New%20Carrollton,MD', 'Washington,DC']\n",
    "\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'New_London_CT', 'New_Haven_CT', 'Stamford_CT', \n",
    "             'Manhattan_NY', 'Newark_NJ', 'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE', 'Baltimore_MD', \n",
    "             'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime='\n",
    "\n",
    "for location, filename in zip(locations, location_names_for_files):\n",
    "    print('Running urls for', location)\n",
    "    for startdate, enddate in dates_list:\n",
    "        url = url_base + startdate + 'T00:00:00&endDateTime=' + enddate + 'T00:00:00&unitGroup=us&contentType=csv&location=' + location + '&key='+os.environ.get('VC_TOKEN')\n",
    "        csv_bytes = requests.get(url).content\n",
    "        filestring = './data/weather_original/' + filename + '_weather_data_' + startdate + '_' + enddate + '.csv'\n",
    "        with open(filestring, 'w', newline='\\n') as csvfile:\n",
    "            csvfile.write(csv_bytes.decode())\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = [('2020-01-01','2020-11-31'), \n",
    "              ('2020-12-01', '2021-02-18')] # replace w curr date\n",
    "for location in location_names_for_files:\n",
    "    print('Fixing data for ', location)\n",
    "    startdate1, enddate1 = dates_list[0]\n",
    "    startdate2, enddate2 = dates_list[1]\n",
    "    weather_2020_part1 = pd.read_csv('./data/weather_original/' + filename + '_weather_data_' + startdate1 + '_' + enddate1 + '.csv')\n",
    "    weather_2020_2021_part2 = pd.read_csv('./data/weather_original/' + filename + '_weather_data_' + startdate2 + '_' + enddate2 + '.csv')\n",
    "    full_weather = pd.concat([weather_2020_part1, weather_2020_2021_part2], ignore_index=True, axis=0)\n",
    "    full_weather_new = full_weather[['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                    'Latitude', 'Longitude', 'Conditions']].iloc[:]\n",
    "    nona_df = full_weather_new.replace('', np.nan).dropna()\n",
    "    print(nona_df.shape[0]/full_weather_new.shape[0])\n",
    "    full_weather_new.to_csv('./data/weather/' +  location + '_weather_data_' + startdate1 + '_' + enddate2 + '_col_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw weather data comes well-formatted in CSV already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filestring = './data/weather_original/Boston_MA_weather_data_2011-01-01_2011-12-31.csv'\n",
    "df_sample = pd.read_csv(filestring)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NA values (very few rows are actually dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in location_names_for_files:\n",
    "    for startdate, enddate in dates_list:\n",
    "        filestring = './data/weather_original/' + location + '_weather_data_' + startdate + '_' + enddate + '.csv'\n",
    "        df = pd.read_csv(filestring, usecols=['Address', 'Date time', 'Temperature', 'Precipitation', 'Cloud Cover', \n",
    "                                    'Latitude', 'Longitude', 'Conditions'])\n",
    "        nona_df = df.replace('', np.nan).dropna()\n",
    "        print(nona_df.shape[0]/df.shape[0])\n",
    "        nona_df.to_csv('./data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "        CREATE TABLE weather_hourly (\n",
    "            weather_id SERIAL PRIMARY KEY,\n",
    "            location text DEFAULT NULL,\n",
    "            date_time timestamp DEFAULT NULL,\n",
    "            temperature real DEFAULT NUll,\n",
    "            precipitation real DEFAULT NULL,\n",
    "            cloud_cover real DEFAULT NULL,\n",
    "            latitude real DEFAULT NULL,\n",
    "            longitude real DEFAULT NULL,\n",
    "            conditions text DEFAULT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weather_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO weather_hourly (location, date_time, temperature, precipitation, \n",
    "                   cloud_cover, latitude, longitude, conditions)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                   \n",
    "    with open(csv_file, newline='') as file: \n",
    "        data_reader = csv.reader(file, delimiter=',')\n",
    "        next(data_reader, None)   # skip header                                                                           \n",
    "        for row in data_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(DSN)\n",
    "create_tables(conn)\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for startdate, enddate in dates_list:\n",
    "        csv_file = './data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv'\n",
    "        update_weather_table(conn, csv_file)\n",
    "    print('Finished adding location', location, 'to the database')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://appuser:test@localhost:5432/amtrakproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1327576</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1327576,)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) from weather_hourly;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>783486</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(783486,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) from departures;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "18 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>column_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dataset_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>station_code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>origin_week_day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>full_sched_dep_datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>sched_dep_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>sched_dep_week_day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>sched_dep_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>act_dep_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>depart_diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>service_disruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>cancellations</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('dataset_id',),\n",
       " ('train_num',),\n",
       " ('station_code',),\n",
       " ('direction',),\n",
       " ('origin_date',),\n",
       " ('origin_year',),\n",
       " ('origin_quarter',),\n",
       " ('origin_month',),\n",
       " ('origin_day',),\n",
       " ('origin_week_day',),\n",
       " ('full_sched_dep_datetime',),\n",
       " ('sched_dep_date',),\n",
       " ('sched_dep_week_day',),\n",
       " ('sched_dep_time',),\n",
       " ('act_dep_time',),\n",
       " ('depart_diff',),\n",
       " ('service_disruption',),\n",
       " ('cancellations',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'departures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>table_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>weather_hourly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>arrivals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>departures</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('weather_hourly',), ('train_info',), ('arrivals',), ('departures',)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT table_name \n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE table_schema='public'\n",
    "AND table_type='BASE TABLE';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "13 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>column_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_info_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>operating_direction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_tues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_wed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_thurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>reg_operates_on_sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>depart_origin_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>depart_ny_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>arrive_dest_time</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('train_info_id',),\n",
       " ('train_num',),\n",
       " ('operating_direction',),\n",
       " ('reg_operates_on_mon',),\n",
       " ('reg_operates_on_tues',),\n",
       " ('reg_operates_on_wed',),\n",
       " ('reg_operates_on_thurs',),\n",
       " ('reg_operates_on_fri',),\n",
       " ('reg_operates_on_sat',),\n",
       " ('reg_operates_on_sun',),\n",
       " ('depart_origin_time',),\n",
       " ('depart_ny_time',),\n",
       " ('arrive_dest_time',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'train_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>170</td>\n",
       "        <td>7.2984603639139840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>150</td>\n",
       "        <td>5.9713756858597210</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(170, Decimal('7.2984603639139840')), (150, Decimal('5.9713756858597210'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT d.train_num, AVG(depart_diff) from departures d\n",
    "INNER JOIN (\n",
    "    SELECT *\n",
    "    FROM train_info \n",
    "    WHERE depart_origin_time = 'OVERNIGHT'\n",
    "    ) ti\n",
    "ON d.train_num = ti.train_num\n",
    "GROUP BY d.train_num\n",
    "ORDER BY AVG(depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "12 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>13.5163242554546902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>86</td>\n",
       "        <td>13.4715401629476160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>11.3939365671641791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>161</td>\n",
       "        <td>10.5528681994081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>171</td>\n",
       "        <td>9.9943882483318007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>172</td>\n",
       "        <td>9.3880031364349190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>195</td>\n",
       "        <td>9.2522048666454149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>9.0879536723811708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>95</td>\n",
       "        <td>9.0736019157627835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>99</td>\n",
       "        <td>8.1770228215767635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>160</td>\n",
       "        <td>5.9513522904963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>162</td>\n",
       "        <td>5.7067988668555241</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(83, Decimal('13.5163242554546902')),\n",
       " (86, Decimal('13.4715401629476160')),\n",
       " (93, Decimal('11.3939365671641791')),\n",
       " (161, Decimal('10.5528681994081493')),\n",
       " (171, Decimal('9.9943882483318007')),\n",
       " (172, Decimal('9.3880031364349190')),\n",
       " (195, Decimal('9.2522048666454149')),\n",
       " (164, Decimal('9.0879536723811708')),\n",
       " (95, Decimal('9.0736019157627835')),\n",
       " (99, Decimal('8.1770228215767635')),\n",
       " (160, Decimal('5.9513522904963225')),\n",
       " (162, Decimal('5.7067988668555241'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT d.train_num, AVG(depart_diff) from departures d\n",
    "INNER JOIN (\n",
    "    SELECT *\n",
    "    FROM train_info \n",
    "    WHERE depart_origin_time = 'AM'\n",
    "    ) ti\n",
    "ON d.train_num = ti.train_num\n",
    "GROUP BY d.train_num\n",
    "ORDER BY AVG(depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "8 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>19.9815405431526046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>10.5713442700342264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>175</td>\n",
       "        <td>10.0098284976793825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>139</td>\n",
       "        <td>9.1303370786516854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>165</td>\n",
       "        <td>7.9497769133966365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>167</td>\n",
       "        <td>7.6586041555673948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>168</td>\n",
       "        <td>6.9448518627163391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>132</td>\n",
       "        <td>6.1228404298734866</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(94, Decimal('19.9815405431526046')),\n",
       " (166, Decimal('10.5713442700342264')),\n",
       " (175, Decimal('10.0098284976793825')),\n",
       " (139, Decimal('9.1303370786516854')),\n",
       " (165, Decimal('7.9497769133966365')),\n",
       " (167, Decimal('7.6586041555673948')),\n",
       " (168, Decimal('6.9448518627163391')),\n",
       " (132, Decimal('6.1228404298734866'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT d.train_num, AVG(depart_diff) from departures d\n",
    "INNER JOIN (\n",
    "    SELECT *\n",
    "    FROM train_info \n",
    "    WHERE depart_origin_time = 'PM'\n",
    "    ) ti\n",
    "ON d.train_num = ti.train_num\n",
    "GROUP BY d.train_num\n",
    "ORDER BY AVG(depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "7 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88</td>\n",
       "        <td>14.0034088525277953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>82</td>\n",
       "        <td>13.6169088507265522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>163</td>\n",
       "        <td>10.9038973448911337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>173</td>\n",
       "        <td>9.6344140794131880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>135</td>\n",
       "        <td>8.8651828976092860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174</td>\n",
       "        <td>8.7249259719170885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>137</td>\n",
       "        <td>8.2719068861200841</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(88, Decimal('14.0034088525277953')),\n",
       " (82, Decimal('13.6169088507265522')),\n",
       " (163, Decimal('10.9038973448911337')),\n",
       " (173, Decimal('9.6344140794131880')),\n",
       " (135, Decimal('8.8651828976092860')),\n",
       " (174, Decimal('8.7249259719170885')),\n",
       " (137, Decimal('8.2719068861200841'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT d.train_num, AVG(depart_diff) from departures d\n",
    "INNER JOIN (\n",
    "    SELECT *\n",
    "    FROM train_info \n",
    "    WHERE depart_origin_time = 'MID'\n",
    "    ) ti\n",
    "ON d.train_num = ti.train_num\n",
    "GROUP BY d.train_num\n",
    "ORDER BY AVG(depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "13 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88</td>\n",
       "        <td>14.0034088525277953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>163</td>\n",
       "        <td>10.9038973448911337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>161</td>\n",
       "        <td>10.5528681994081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>10.1217056935047975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>195</td>\n",
       "        <td>9.2522048666454149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>9.0879536723811708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>135</td>\n",
       "        <td>8.8651828976092860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>99</td>\n",
       "        <td>8.1770228215767635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>165</td>\n",
       "        <td>7.9497769133966365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>150</td>\n",
       "        <td>5.9713756858597210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>160</td>\n",
       "        <td>5.9513522904963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>162</td>\n",
       "        <td>5.7067988668555241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>67</td>\n",
       "        <td>4.9086023902961113</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(88, Decimal('14.0034088525277953')),\n",
       " (163, Decimal('10.9038973448911337')),\n",
       " (161, Decimal('10.5528681994081493')),\n",
       " (66, Decimal('10.1217056935047975')),\n",
       " (195, Decimal('9.2522048666454149')),\n",
       " (164, Decimal('9.0879536723811708')),\n",
       " (135, Decimal('8.8651828976092860')),\n",
       " (99, Decimal('8.1770228215767635')),\n",
       " (165, Decimal('7.9497769133966365')),\n",
       " (150, Decimal('5.9713756858597210')),\n",
       " (160, Decimal('5.9513522904963225')),\n",
       " (162, Decimal('5.7067988668555241')),\n",
       " (67, Decimal('4.9086023902961113'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT d.train_num, AVG(depart_diff)\n",
    "FROM departures d\n",
    "INNER JOIN (\n",
    "    SELECT * \n",
    "    FROM train_info\n",
    "    WHERE reg_operates_on_sat = 't' AND reg_operates_on_sun = 't'\n",
    "    ) ti\n",
    "ON d.train_num = ti.train_num\n",
    "GROUP BY d.train_num\n",
    "ORDER BY AVG(depart_diff) DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "13 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>depart_origin_time</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>PM</td>\n",
       "        <td>19.9815405431526046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>86</td>\n",
       "        <td>AM</td>\n",
       "        <td>13.4715401629476160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>AM</td>\n",
       "        <td>11.3939365671641791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>EVENING</td>\n",
       "        <td>10.1217056935047975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>175</td>\n",
       "        <td>PM</td>\n",
       "        <td>10.0098284976793825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>171</td>\n",
       "        <td>AM</td>\n",
       "        <td>9.9943882483318007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>173</td>\n",
       "        <td>MID</td>\n",
       "        <td>9.6344140794131880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>172</td>\n",
       "        <td>AM</td>\n",
       "        <td>9.3880031364349190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>95</td>\n",
       "        <td>AM</td>\n",
       "        <td>9.0736019157627835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>174</td>\n",
       "        <td>MID</td>\n",
       "        <td>8.7249259719170885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>137</td>\n",
       "        <td>MID</td>\n",
       "        <td>8.2719068861200841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>170</td>\n",
       "        <td>OVERNIGHT</td>\n",
       "        <td>7.2984603639139840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>67</td>\n",
       "        <td>EVENING</td>\n",
       "        <td>4.9086023902961113</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(94, 'PM', Decimal('19.9815405431526046')),\n",
       " (86, 'AM', Decimal('13.4715401629476160')),\n",
       " (93, 'AM', Decimal('11.3939365671641791')),\n",
       " (66, 'EVENING', Decimal('10.1217056935047975')),\n",
       " (175, 'PM', Decimal('10.0098284976793825')),\n",
       " (171, 'AM', Decimal('9.9943882483318007')),\n",
       " (173, 'MID', Decimal('9.6344140794131880')),\n",
       " (172, 'AM', Decimal('9.3880031364349190')),\n",
       " (95, 'AM', Decimal('9.0736019157627835')),\n",
       " (174, 'MID', Decimal('8.7249259719170885')),\n",
       " (137, 'MID', Decimal('8.2719068861200841')),\n",
       " (170, 'OVERNIGHT', Decimal('7.2984603639139840')),\n",
       " (67, 'EVENING', Decimal('4.9086023902961113'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT  d.train_Num, ti.depart_origin_time, AVG(d.depart_diff)\n",
    "FROM train_info ti\n",
    "INNER JOIN(\n",
    "    SELECT d.train_num, d.depart_diff \n",
    "    FROM departures d\n",
    ") AS d\n",
    "ON ti.train_num = d.train_num\n",
    "WHERE reg_operates_on_mon = 't' AND reg_operates_on_thurs = 't'\n",
    "GROUP BY d.train_num, ti.depart_origin_time\n",
    "ORDER BY AVG(d.depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "13 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>depart_origin_time</th>\n",
       "        <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88</td>\n",
       "        <td>MID</td>\n",
       "        <td>14.0034088525277953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>163</td>\n",
       "        <td>MID</td>\n",
       "        <td>10.9038973448911337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>161</td>\n",
       "        <td>AM</td>\n",
       "        <td>10.5528681994081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>EVENING</td>\n",
       "        <td>10.1217056935047975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>195</td>\n",
       "        <td>AM</td>\n",
       "        <td>9.2522048666454149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>AM</td>\n",
       "        <td>9.0879536723811708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>135</td>\n",
       "        <td>MID</td>\n",
       "        <td>8.8651828976092860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>99</td>\n",
       "        <td>AM</td>\n",
       "        <td>8.1770228215767635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>165</td>\n",
       "        <td>PM</td>\n",
       "        <td>7.9497769133966365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>150</td>\n",
       "        <td>OVERNIGHT</td>\n",
       "        <td>5.9713756858597210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>160</td>\n",
       "        <td>AM</td>\n",
       "        <td>5.9513522904963225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>162</td>\n",
       "        <td>AM</td>\n",
       "        <td>5.7067988668555241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>67</td>\n",
       "        <td>EVENING</td>\n",
       "        <td>4.9086023902961113</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(88, 'MID', Decimal('14.0034088525277953')),\n",
       " (163, 'MID', Decimal('10.9038973448911337')),\n",
       " (161, 'AM', Decimal('10.5528681994081493')),\n",
       " (66, 'EVENING', Decimal('10.1217056935047975')),\n",
       " (195, 'AM', Decimal('9.2522048666454149')),\n",
       " (164, 'AM', Decimal('9.0879536723811708')),\n",
       " (135, 'MID', Decimal('8.8651828976092860')),\n",
       " (99, 'AM', Decimal('8.1770228215767635')),\n",
       " (165, 'PM', Decimal('7.9497769133966365')),\n",
       " (150, 'OVERNIGHT', Decimal('5.9713756858597210')),\n",
       " (160, 'AM', Decimal('5.9513522904963225')),\n",
       " (162, 'AM', Decimal('5.7067988668555241')),\n",
       " (67, 'EVENING', Decimal('4.9086023902961113'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT  d.train_Num, ti.depart_origin_time, AVG(d.depart_diff)\n",
    "FROM train_info ti\n",
    "INNER JOIN(\n",
    "    SELECT d.train_num, d.depart_diff \n",
    "    FROM departures d\n",
    ") AS d\n",
    "ON ti.train_num = d.train_num\n",
    "WHERE reg_operates_on_sat = 't' AND reg_operates_on_sun = 't'\n",
    "GROUP BY d.train_num, ti.depart_origin_time\n",
    "ORDER BY AVG(d.depart_diff) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "9 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>column_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>weather_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>date_time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>precipitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>cloud_cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>latitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>longitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>conditions</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('weather_id',),\n",
       " ('location',),\n",
       " ('date_time',),\n",
       " ('temperature',),\n",
       " ('precipitation',),\n",
       " ('cloud_cover',),\n",
       " ('latitude',),\n",
       " ('longitude',),\n",
       " ('conditions',)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'weather_hourly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "(psycopg2.errors.SyntaxError) syntax error at or near \"####\"\n",
      "LINE 1: #### IN PROGRESS SELECT * \n",
      "        ^\n",
      "\n",
      "[SQL: #### IN PROGRESS SELECT * \n",
      "FROM departures d\n",
      "INNER JOIN (\n",
      "    SELECT * \n",
      "    FROM weather_hourly\n",
      "    WHERE location = 'Providence,RI'\n",
      ") wh\n",
      "ON DATE(d.full_sched_dep_datetime) = DATE(wh.date_time)\n",
      "AND \n",
      "WHERE (d.full_sched_dep_datetime - interval '30 minutes', d.full_sched_dep_datetime + interval '30 minutes') \n",
      "OVERLAPS (wh.date_time, wh.date_time + interval '1 hour')]\n",
      "(Background on this error at: http://sqlalche.me/e/13/f405)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "#### IN PROGRESS\n",
    "SELECT * \n",
    "FROM departures d\n",
    "INNER JOIN (\n",
    "    SELECT * \n",
    "    FROM weather_hourly\n",
    "    WHERE location = 'Providence,RI'\n",
    ") wh\n",
    "ON DATE(d.full_sched_dep_datetime) = DATE(wh.date_time)\n",
    "AND \n",
    "WHERE (d.full_sched_dep_datetime - interval '30 minutes', d.full_sched_dep_datetime + interval '30 minutes') \n",
    "OVERLAPS (wh.date_time, wh.date_time + interval '1 hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_table(conn):\n",
    "    \"\"\"Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [  \n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS test_table CASCADE;\n",
    "        CREATE TABLE test_table (\n",
    "            weather_id SERIAL PRIMARY KEY,\n",
    "            location text DEFAULT NULL,\n",
    "            date_time timestamp DEFAULT NULL,\n",
    "            temperature real DEFAULT NUll,\n",
    "            precipitation real DEFAULT NULL,\n",
    "            cloud_cover real DEFAULT NULL,\n",
    "            latitude real DEFAULT NULL,\n",
    "            longitude real DEFAULT NULL,\n",
    "            conditions text DEFAULT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "    try:\n",
    "        conn = psycopg2.connect(DSN)\n",
    "        cur = conn.cursor()\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        err_type, err_obj, traceback = sys.exc_info()\n",
    "        line_num = traceback.tb_lineno\n",
    "        print (\"\\npsycopg2 ERROR:\", error, \"on line number:\", line_num)\n",
    "        print (\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_table(conn, csv_file):\n",
    "    c = conn.cursor()\n",
    "    commands = [\"\"\"INSERT INTO test_table (location, date_time, temperature, precipitation, \n",
    "                   cloud_cover, latitude, longitude, conditions)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                   ON CONFLICT DO NOTHING\"\"\"]                \n",
    "                   \n",
    "    with open(csv_file, newline='') as file: \n",
    "        data_reader = csv.reader(file, delimiter=',')\n",
    "        next(data_reader, None)   # skip header                                                                           \n",
    "        for row in data_reader:                                           \n",
    "            try:\n",
    "                c.execute(commands[0], tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                print(row)\n",
    "                conn.rollback()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSN = \"dbname='amtrakproject' user='appuser' password={}\".format(os.environ.get('DB_PASS'))\n",
    "conn = psycopg2.connect(DSN)\n",
    "create_test_table(conn)\n",
    "for location in ['Boston_MA']:\n",
    "    start = time.time()\n",
    "    for startdate, enddate in dates_list:\n",
    "        csv_file = './data/weather/' + location + '_weather_data_' + startdate + '_' + enddate + '_col_subset.csv'\n",
    "        update_test_table(conn, csv_file)\n",
    "print('Finished adding location', location, 'to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "1327576 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "UPDATE weather_hourly\n",
    "SET location = REPLACE(location, ',' , ', ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "15 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Baltimore BWI Airport, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Baltimore, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Kingston, RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Manhattan, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>New Carrollton, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>New Haven, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>New London, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Newark, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Providence, RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Stamford, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Trenton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Washington, DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Wilmington, DE</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Baltimore BWI Airport, MD',),\n",
       " ('Baltimore, MD',),\n",
       " ('Boston, MA',),\n",
       " ('Kingston, RI',),\n",
       " ('Manhattan, NY',),\n",
       " ('New Carrollton, MD',),\n",
       " ('New Haven, CT',),\n",
       " ('New London, CT',),\n",
       " ('Newark, NJ',),\n",
       " ('Philadelphia, PA',),\n",
       " ('Providence, RI',),\n",
       " ('Stamford, CT',),\n",
       " ('Trenton, NJ',),\n",
       " ('Washington, DC',),\n",
       " ('Wilmington, DE',)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT location FROM weather_hourly\n",
    "GROUP BY location;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "30 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>origin_quarter</th>\n",
       "        <th>station_code</th>\n",
       "        <th>avg_delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>3</td>\n",
       "        <td>NCR</td>\n",
       "        <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>2</td>\n",
       "        <td>NCR</td>\n",
       "        <td>34.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>STM</td>\n",
       "        <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>PVD</td>\n",
       "        <td>31.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>NLC</td>\n",
       "        <td>31.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>PVD</td>\n",
       "        <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>3</td>\n",
       "        <td>BWI</td>\n",
       "        <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>KIN</td>\n",
       "        <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>3</td>\n",
       "        <td>NCR</td>\n",
       "        <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>NLC</td>\n",
       "        <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>KIN</td>\n",
       "        <td>29.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>STM</td>\n",
       "        <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>NWK</td>\n",
       "        <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>4</td>\n",
       "        <td>PVD</td>\n",
       "        <td>28.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>173</td>\n",
       "        <td>3</td>\n",
       "        <td>NCR</td>\n",
       "        <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>2</td>\n",
       "        <td>BWI</td>\n",
       "        <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>2</td>\n",
       "        <td>NCR</td>\n",
       "        <td>28.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>139</td>\n",
       "        <td>3</td>\n",
       "        <td>NCR</td>\n",
       "        <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>2</td>\n",
       "        <td>TRE</td>\n",
       "        <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>3</td>\n",
       "        <td>BAL</td>\n",
       "        <td>27.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>NHV</td>\n",
       "        <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>NWK</td>\n",
       "        <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>4</td>\n",
       "        <td>NLC</td>\n",
       "        <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>4</td>\n",
       "        <td>NCR</td>\n",
       "        <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>3</td>\n",
       "        <td>BWI</td>\n",
       "        <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>4</td>\n",
       "        <td>KIN</td>\n",
       "        <td>26.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>4</td>\n",
       "        <td>STM</td>\n",
       "        <td>25.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>1</td>\n",
       "        <td>NCR</td>\n",
       "        <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2</td>\n",
       "        <td>NHV</td>\n",
       "        <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>3</td>\n",
       "        <td>NYP</td>\n",
       "        <td>25.6</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(83, 3, 'NCR', Decimal('34.8')),\n",
       " (83, 2, 'NCR', Decimal('34.4')),\n",
       " (94, 3, 'STM', Decimal('33.3')),\n",
       " (94, 3, 'PVD', Decimal('31.9')),\n",
       " (94, 3, 'NLC', Decimal('31.8')),\n",
       " (94, 2, 'PVD', Decimal('31.0')),\n",
       " (83, 3, 'BWI', Decimal('30.9')),\n",
       " (94, 3, 'KIN', Decimal('30.9')),\n",
       " (93, 3, 'NCR', Decimal('30.4')),\n",
       " (94, 2, 'NLC', Decimal('30.1')),\n",
       " (94, 2, 'KIN', Decimal('29.5')),\n",
       " (94, 2, 'STM', Decimal('29.4')),\n",
       " (94, 3, 'NWK', Decimal('29.0')),\n",
       " (94, 4, 'PVD', Decimal('28.9')),\n",
       " (173, 3, 'NCR', Decimal('28.2')),\n",
       " (83, 2, 'BWI', Decimal('28.2')),\n",
       " (93, 2, 'NCR', Decimal('28.1')),\n",
       " (139, 3, 'NCR', Decimal('28.0')),\n",
       " (66, 2, 'TRE', Decimal('27.8')),\n",
       " (83, 3, 'BAL', Decimal('27.6')),\n",
       " (94, 3, 'NHV', Decimal('26.9')),\n",
       " (94, 2, 'NWK', Decimal('26.7')),\n",
       " (94, 4, 'NLC', Decimal('26.5')),\n",
       " (83, 4, 'NCR', Decimal('26.5')),\n",
       " (93, 3, 'BWI', Decimal('26.4')),\n",
       " (94, 4, 'KIN', Decimal('26.3')),\n",
       " (94, 4, 'STM', Decimal('25.9')),\n",
       " (83, 1, 'NCR', Decimal('25.8')),\n",
       " (94, 2, 'NHV', Decimal('25.8')),\n",
       " (94, 3, 'NYP', Decimal('25.6'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT  d.train_num, d.origin_quarter, d.station_code, ROUND( CAST( AVG(d.depart_diff) as numeric), 1) as avg_delay\n",
    "FROM departures d\n",
    "GROUP BY d.origin_quarter, d.train_num, d.station_code\n",
    "ORDER BY ROUND( CAST( AVG(d.depart_diff) as numeric), 2) DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "30 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>origin_month</th>\n",
       "        <th>station_code</th>\n",
       "        <th>avg_delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>STM</td>\n",
       "        <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>PVD</td>\n",
       "        <td>40.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>NLC</td>\n",
       "        <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>6</td>\n",
       "        <td>NCR</td>\n",
       "        <td>38.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>KIN</td>\n",
       "        <td>38.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>7</td>\n",
       "        <td>NCR</td>\n",
       "        <td>38.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>9</td>\n",
       "        <td>NCR</td>\n",
       "        <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>6</td>\n",
       "        <td>KIN</td>\n",
       "        <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>139</td>\n",
       "        <td>9</td>\n",
       "        <td>NCR</td>\n",
       "        <td>37.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>6</td>\n",
       "        <td>PVD</td>\n",
       "        <td>37.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>139</td>\n",
       "        <td>11</td>\n",
       "        <td>NCR</td>\n",
       "        <td>36.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>6</td>\n",
       "        <td>PVD</td>\n",
       "        <td>36.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>NWK</td>\n",
       "        <td>36.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>11</td>\n",
       "        <td>NCR</td>\n",
       "        <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>6</td>\n",
       "        <td>STM</td>\n",
       "        <td>35.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>6</td>\n",
       "        <td>NLC</td>\n",
       "        <td>35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>6</td>\n",
       "        <td>KIN</td>\n",
       "        <td>35.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>8</td>\n",
       "        <td>NCR</td>\n",
       "        <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>9</td>\n",
       "        <td>BWI</td>\n",
       "        <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>5</td>\n",
       "        <td>NCR</td>\n",
       "        <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88</td>\n",
       "        <td>6</td>\n",
       "        <td>PVD</td>\n",
       "        <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>7</td>\n",
       "        <td>NCR</td>\n",
       "        <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>NHV</td>\n",
       "        <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>7</td>\n",
       "        <td>BWI</td>\n",
       "        <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88</td>\n",
       "        <td>6</td>\n",
       "        <td>KIN</td>\n",
       "        <td>32.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>11</td>\n",
       "        <td>PVD</td>\n",
       "        <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>NYP</td>\n",
       "        <td>32.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>7</td>\n",
       "        <td>RTE</td>\n",
       "        <td>32.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>11</td>\n",
       "        <td>BWI</td>\n",
       "        <td>32.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>93</td>\n",
       "        <td>6</td>\n",
       "        <td>NCR</td>\n",
       "        <td>31.9</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(94, 7, 'STM', Decimal('40.6')),\n",
       " (94, 7, 'PVD', Decimal('40.5')),\n",
       " (94, 7, 'NLC', Decimal('39.5')),\n",
       " (83, 6, 'NCR', Decimal('38.4')),\n",
       " (94, 7, 'KIN', Decimal('38.3')),\n",
       " (83, 7, 'NCR', Decimal('38.3')),\n",
       " (83, 9, 'NCR', Decimal('38.0')),\n",
       " (166, 6, 'KIN', Decimal('37.9')),\n",
       " (139, 9, 'NCR', Decimal('37.3')),\n",
       " (166, 6, 'PVD', Decimal('37.1')),\n",
       " (139, 11, 'NCR', Decimal('36.8')),\n",
       " (94, 6, 'PVD', Decimal('36.7')),\n",
       " (94, 7, 'NWK', Decimal('36.3')),\n",
       " (83, 11, 'NCR', Decimal('35.9')),\n",
       " (94, 6, 'STM', Decimal('35.7')),\n",
       " (94, 6, 'NLC', Decimal('35.5')),\n",
       " (94, 6, 'KIN', Decimal('35.3')),\n",
       " (93, 8, 'NCR', Decimal('34.5')),\n",
       " (83, 9, 'BWI', Decimal('34.5')),\n",
       " (83, 5, 'NCR', Decimal('33.9')),\n",
       " (88, 6, 'PVD', Decimal('33.9')),\n",
       " (93, 7, 'NCR', Decimal('33.9')),\n",
       " (94, 7, 'NHV', Decimal('33.2')),\n",
       " (83, 7, 'BWI', Decimal('33.0')),\n",
       " (88, 6, 'KIN', Decimal('32.9')),\n",
       " (94, 11, 'PVD', Decimal('32.4')),\n",
       " (94, 7, 'NYP', Decimal('32.3')),\n",
       " (94, 7, 'RTE', Decimal('32.1')),\n",
       " (83, 11, 'BWI', Decimal('32.1')),\n",
       " (93, 6, 'NCR', Decimal('31.9'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT  d.train_num, d.origin_month, d.station_code, ROUND( CAST( AVG(d.depart_diff) as numeric), 1) as avg_delay\n",
    "FROM departures d\n",
    "GROUP BY d.origin_month, d.train_num, d.station_code\n",
    "ORDER BY ROUND( CAST( AVG(d.depart_diff) as numeric), 2) DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "30 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>origin_year</th>\n",
       "        <th>station_code</th>\n",
       "        <th>avg_delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2012</td>\n",
       "        <td>KIN</td>\n",
       "        <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>2018</td>\n",
       "        <td>TRE</td>\n",
       "        <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2011</td>\n",
       "        <td>PVD</td>\n",
       "        <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2014</td>\n",
       "        <td>RTE</td>\n",
       "        <td>53.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>PVD</td>\n",
       "        <td>50.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>STM</td>\n",
       "        <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>PVD</td>\n",
       "        <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2012</td>\n",
       "        <td>BBY</td>\n",
       "        <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>STM</td>\n",
       "        <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>KIN</td>\n",
       "        <td>45.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NLC</td>\n",
       "        <td>44.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NWK</td>\n",
       "        <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>KIN</td>\n",
       "        <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NWK</td>\n",
       "        <td>43.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>NLC</td>\n",
       "        <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NLC</td>\n",
       "        <td>43.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NHV</td>\n",
       "        <td>42.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NHV</td>\n",
       "        <td>42.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2014</td>\n",
       "        <td>KIN</td>\n",
       "        <td>42.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>KIN</td>\n",
       "        <td>42.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>PVD</td>\n",
       "        <td>41.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>2014</td>\n",
       "        <td>NCR</td>\n",
       "        <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2012</td>\n",
       "        <td>PVD</td>\n",
       "        <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2013</td>\n",
       "        <td>KIN</td>\n",
       "        <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>RTE</td>\n",
       "        <td>40.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NYP</td>\n",
       "        <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>RTE</td>\n",
       "        <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>BBY</td>\n",
       "        <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>BBY</td>\n",
       "        <td>38.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NYP</td>\n",
       "        <td>38.7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(166, 2012, 'KIN', Decimal('119.0')),\n",
       " (66, 2018, 'TRE', Decimal('84.9')),\n",
       " (166, 2011, 'PVD', Decimal('62.0')),\n",
       " (164, 2014, 'RTE', Decimal('53.5')),\n",
       " (94, 2014, 'PVD', Decimal('50.2')),\n",
       " (94, 2015, 'STM', Decimal('49.9')),\n",
       " (94, 2015, 'PVD', Decimal('49.7')),\n",
       " (164, 2012, 'BBY', Decimal('48.3')),\n",
       " (94, 2014, 'STM', Decimal('48.0')),\n",
       " (94, 2014, 'KIN', Decimal('45.9')),\n",
       " (94, 2014, 'NLC', Decimal('44.8')),\n",
       " (94, 2015, 'NWK', Decimal('44.7')),\n",
       " (94, 2015, 'KIN', Decimal('44.5')),\n",
       " (94, 2014, 'NWK', Decimal('43.8')),\n",
       " (94, 2013, 'NLC', Decimal('43.5')),\n",
       " (94, 2015, 'NLC', Decimal('43.4')),\n",
       " (94, 2014, 'NHV', Decimal('42.8')),\n",
       " (94, 2015, 'NHV', Decimal('42.7')),\n",
       " (164, 2014, 'KIN', Decimal('42.5')),\n",
       " (94, 2013, 'KIN', Decimal('42.3')),\n",
       " (94, 2013, 'PVD', Decimal('41.6')),\n",
       " (83, 2014, 'NCR', Decimal('41.0')),\n",
       " (166, 2012, 'PVD', Decimal('41.0')),\n",
       " (166, 2013, 'KIN', Decimal('40.6')),\n",
       " (94, 2014, 'RTE', Decimal('40.4')),\n",
       " (94, 2015, 'NYP', Decimal('40.2')),\n",
       " (94, 2015, 'RTE', Decimal('40.2')),\n",
       " (94, 2015, 'BBY', Decimal('39.5')),\n",
       " (94, 2014, 'BBY', Decimal('38.9')),\n",
       " (94, 2014, 'NYP', Decimal('38.7'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT  d.train_num, d.origin_year, d.station_code, ROUND( CAST( AVG(d.depart_diff) as numeric), 1) as avg_delay\n",
    "FROM departures d\n",
    "GROUP BY d.origin_year, d.train_num, d.station_code\n",
    "ORDER BY ROUND( CAST( AVG(d.depart_diff) as numeric), 2) DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://appuser:***@localhost:5432/amtrakproject\n",
      "30 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>train_num</th>\n",
       "        <th>origin_year</th>\n",
       "        <th>station_code</th>\n",
       "        <th>avg_delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2012</td>\n",
       "        <td>KIN</td>\n",
       "        <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>66</td>\n",
       "        <td>2018</td>\n",
       "        <td>TRE</td>\n",
       "        <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2011</td>\n",
       "        <td>PVD</td>\n",
       "        <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2014</td>\n",
       "        <td>RTE</td>\n",
       "        <td>53.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>PVD</td>\n",
       "        <td>50.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>STM</td>\n",
       "        <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>PVD</td>\n",
       "        <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2012</td>\n",
       "        <td>BBY</td>\n",
       "        <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>STM</td>\n",
       "        <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>KIN</td>\n",
       "        <td>45.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NLC</td>\n",
       "        <td>44.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NWK</td>\n",
       "        <td>44.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>KIN</td>\n",
       "        <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NWK</td>\n",
       "        <td>43.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>NLC</td>\n",
       "        <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NLC</td>\n",
       "        <td>43.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NHV</td>\n",
       "        <td>42.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NHV</td>\n",
       "        <td>42.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>164</td>\n",
       "        <td>2014</td>\n",
       "        <td>KIN</td>\n",
       "        <td>42.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>KIN</td>\n",
       "        <td>42.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2013</td>\n",
       "        <td>PVD</td>\n",
       "        <td>41.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>83</td>\n",
       "        <td>2014</td>\n",
       "        <td>NCR</td>\n",
       "        <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2012</td>\n",
       "        <td>PVD</td>\n",
       "        <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>166</td>\n",
       "        <td>2013</td>\n",
       "        <td>KIN</td>\n",
       "        <td>40.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>RTE</td>\n",
       "        <td>40.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>NYP</td>\n",
       "        <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>RTE</td>\n",
       "        <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2015</td>\n",
       "        <td>BBY</td>\n",
       "        <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>BBY</td>\n",
       "        <td>38.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>94</td>\n",
       "        <td>2014</td>\n",
       "        <td>NYP</td>\n",
       "        <td>38.7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(166, 2012, 'KIN', Decimal('119.0')),\n",
       " (66, 2018, 'TRE', Decimal('84.9')),\n",
       " (166, 2011, 'PVD', Decimal('62.0')),\n",
       " (164, 2014, 'RTE', Decimal('53.5')),\n",
       " (94, 2014, 'PVD', Decimal('50.2')),\n",
       " (94, 2015, 'STM', Decimal('49.9')),\n",
       " (94, 2015, 'PVD', Decimal('49.7')),\n",
       " (164, 2012, 'BBY', Decimal('48.3')),\n",
       " (94, 2014, 'STM', Decimal('48.0')),\n",
       " (94, 2014, 'KIN', Decimal('45.9')),\n",
       " (94, 2014, 'NLC', Decimal('44.8')),\n",
       " (94, 2015, 'NWK', Decimal('44.7')),\n",
       " (94, 2015, 'KIN', Decimal('44.5')),\n",
       " (94, 2014, 'NWK', Decimal('43.8')),\n",
       " (94, 2013, 'NLC', Decimal('43.5')),\n",
       " (94, 2015, 'NLC', Decimal('43.4')),\n",
       " (94, 2014, 'NHV', Decimal('42.8')),\n",
       " (94, 2015, 'NHV', Decimal('42.7')),\n",
       " (164, 2014, 'KIN', Decimal('42.5')),\n",
       " (94, 2013, 'KIN', Decimal('42.3')),\n",
       " (94, 2013, 'PVD', Decimal('41.6')),\n",
       " (83, 2014, 'NCR', Decimal('41.0')),\n",
       " (166, 2012, 'PVD', Decimal('41.0')),\n",
       " (166, 2013, 'KIN', Decimal('40.6')),\n",
       " (94, 2014, 'RTE', Decimal('40.4')),\n",
       " (94, 2015, 'NYP', Decimal('40.2')),\n",
       " (94, 2015, 'RTE', Decimal('40.2')),\n",
       " (94, 2015, 'BBY', Decimal('39.5')),\n",
       " (94, 2014, 'BBY', Decimal('38.9')),\n",
       " (94, 2014, 'NYP', Decimal('38.7'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT  d.train_num, d.origin_year, d.station_code, ROUND( CAST( AVG(d.depart_diff) as numeric), 1) as avg_delay\n",
    "FROM departures d\n",
    "GROUP BY d.origin_year, d.train_num, d.station_code\n",
    "ORDER BY ROUND( CAST( AVG(d.depart_diff) as numeric), 2) DESC\n",
    "LIMIT 30;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
