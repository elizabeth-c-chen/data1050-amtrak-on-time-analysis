{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b64903",
   "metadata": {},
   "source": [
    "# ETL and EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bd0e4",
   "metadata": {},
   "source": [
    "# Part 1 - Amtrak Northeast Regional Train Data\n",
    "* This project would not be possible without the diligent joint effort by [Chris Juckins](https://juckins.net/index.php) and [John Bobinyec](http://dixielandsoftware.net/Amtrak/status/StatusMaps/) to collect and preserve Amtrak's on-time performance records. Chris Juckins' archive of timetables was another invaluable resource which enabled me to sort through the trains and stations I chose to use in this project.\n",
    "* The train data is sourced from [Amtrak Status Maps Archive Database (ASMAD)](https://juckins.net/amtrak_status/archive/html/home.php), and has been retrieved with Chris' permission.\n",
    "\n",
    "### Overview of the Process\n",
    "* Functions were written to scrape the HTML table returned from the search query and to process each column to the desired format\n",
    "* Additional columns were also added during processing to aid in joining the train data with weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770400b6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from trains_retrieve_and_process_data import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bee1b4",
   "metadata": {},
   "source": [
    "### Retrieve HTML table data and recreate as a Pandas DataFrame\n",
    "* Default is to collect data from the previous day (run after 5am or else no data will be retrieved, ASMAD updates around 4am)\n",
    "* Collects both arrival and departure data and stores in a dictionary further indexed by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2021,4,21)\n",
    "end = date(2021,4,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = retrieve_data(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd909dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "depart =  raw_data_to_raw_df(raw_data, 'Depart')\n",
    "print(depart.shape[0])\n",
    "depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = raw_data_to_raw_df(raw_data, 'Arrive')\n",
    "print(arrive.shape[0])\n",
    "arrive.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35503c7",
   "metadata": {},
   "source": [
    "### Save the raw DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring = './data/trains_raw/arrive_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "depart_filestring = './data/trains_raw/depart_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "\n",
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd1957",
   "metadata": {},
   "source": [
    "### Process the raw DF with modifications/additions \n",
    "* Modifications to the data:\n",
    "    * Separate the Origin Date and Origin Week Day  into two columns\n",
    "    * Add separate columns for Origin Year and Origin Month\n",
    "    * Separate the Scheduled Arrival/Departure Date, Scheduled Arrival/Departure Week Day, and Scheduled Arrival/Departure Time into three seperate columns\n",
    "    * Calculate the value of the time difference between Scheduled and Actual Arrival/Departure\n",
    "    * Convert Service Disruption and Cancellation column text flags to binary indicator columns\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a9f09",
   "metadata": {},
   "source": [
    "### For new 2021 data, concatenate with previously retrieved and processed data from this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b52e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring2021 = './data/trains/arrive_2021_processed.csv'\n",
    "depart_filestring2021 = './data/trains/depart_2021_processed.csv'\n",
    "        \n",
    "prev_arrive2021 = pd.read_csv(arrive_filestring2021)\n",
    "prev_depart2021 = pd.read_csv(depart_filestring2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021 = pd.concat([prev_arrive2021, full_arrive], ignore_index=True, axis=0)\n",
    "new_depart2021 = pd.concat([prev_depart2021, full_depart], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8436b",
   "metadata": {},
   "source": [
    "### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95aee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a231e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.to_csv(arrive_filestring2021, line_terminator='\\n', index=False)\n",
    "new_depart2021.to_csv(depart_filestring2021, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdecb55",
   "metadata": {},
   "source": [
    "# Part 2 - Visual Crossing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe05c71",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f27186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a70afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_retrieve_and_process_data import *\n",
    "assert os.environ.get('VC_TOKEN') is not None , 'empty token!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068b6ac",
   "metadata": {},
   "source": [
    "### Retrieve unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str(date.today()-timedelta(days=1))\n",
    "end = str(date.today()-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc081561",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_retrievals = retrieve_weather_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36c337",
   "metadata": {},
   "source": [
    "### Data Cleaning/Taking Subset of Columns\n",
    "\n",
    "* Processing recent data by year - add new columns, make minor fixes to string format, take subset of full columns list.\n",
    "* Function processes the files that were successfully created in the previous step.\n",
    "* This part is assuming 2021 data is being read and concatenates the previously retrieved data with the new data to create a single combined file.\n",
    "* Output shows the fraction of the data kept, data is valid and complete almost always ($> 99\\%$ of original data has been retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_weather_data(files_to_process=successful_retrievals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f95a2c",
   "metadata": {},
   "source": [
    "### Data sample for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/weather/Providence_RI_weather_2021_subset.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8693d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9be4bd",
   "metadata": {},
   "source": [
    "# Part 3a: Loading Data into Postgres Database (Composite Primary Key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca8a15",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "import time\n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8e2da",
   "metadata": {},
   "source": [
    "#### Functions to create and update tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, command):\n",
    "    \"\"\"\n",
    "    Create a table in the PostgreSQL database from the specified command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "def update_table(conn, command, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from a CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "        \n",
    "def update_trains(conn, command, arr_or_dep, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from trains CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple([arr_or_dep] + row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99334dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_station_info_table_command = \"\"\" \n",
    "                                    DROP TABLE IF EXISTS station_info CASCADE;\n",
    "\n",
    "                                    CREATE TABLE station_info (\n",
    "                                        station_code text PRIMARY KEY,\n",
    "                                        station_name text,\n",
    "                                        state text,\n",
    "                                        amtrak_city text,\n",
    "                                        weather_loc text,\n",
    "                                        longitude real,\n",
    "                                        latitude real,\n",
    "                                        nb_mile numeric,\n",
    "                                        sb_mile numeric\n",
    "                                    );\n",
    "                                    \"\"\"\n",
    "\n",
    "insert_into_station_info_table_command = \"\"\"\n",
    "                                         INSERT INTO station_info (\n",
    "                                             station_code,\n",
    "                                             station_name,\n",
    "                                             state,\n",
    "                                             amtrak_city,\n",
    "                                             weather_loc,\n",
    "                                             longitude,\n",
    "                                             latitude,\n",
    "                                             nb_mile,\n",
    "                                             sb_mile\n",
    "                                        )\n",
    "                                        VALUES\n",
    "                                            (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                        ON CONFLICT DO NOTHING;\n",
    "                                        \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trains_table_command = \"\"\" \n",
    "                              DROP TABLE IF EXISTS all_trains CASCADE;\n",
    "                              CREATE TABLE all_trains (\n",
    "                                  arr_or_dep text,\n",
    "                                  train_num text,\n",
    "                                  station_code text REFERENCES station_info (station_code), \n",
    "                                  direction text,\n",
    "                                  origin_date date,\n",
    "                                  origin_year int,\n",
    "                                  origin_month int,\n",
    "                                  origin_week_day text,\n",
    "                                  full_sched_arr_dep_datetime timestamp,\n",
    "                                  sched_arr_dep_date date,\n",
    "                                  sched_arr_dep_week_day text,\n",
    "                                  sched_arr_dep_time time,\n",
    "                                  act_arr_dep_time time,\n",
    "                                  full_act_arr_dep_datetime timestamp,\n",
    "                                  timedelta_from_sched numeric,\n",
    "                                  service_disruption boolean,\n",
    "                                  cancellations boolean,\n",
    "                                  PRIMARY KEY (train_num, station_code, origin_date)\n",
    "                              );\n",
    "                              \"\"\"\n",
    "\n",
    "insert_into_trains_table_command = \"\"\"\n",
    "                                     INSERT INTO all_trains (\n",
    "                                          arr_or_dep,\n",
    "                                          train_num,\n",
    "                                          station_code,\n",
    "                                          direction,\n",
    "                                          origin_date,\n",
    "                                          origin_year,\n",
    "                                          origin_month,\n",
    "                                          origin_week_day,\n",
    "                                          full_sched_arr_dep_datetime,\n",
    "                                          sched_arr_dep_date,\n",
    "                                          sched_arr_dep_week_day,\n",
    "                                          sched_arr_dep_time,\n",
    "                                          act_arr_dep_time,\n",
    "                                          full_act_arr_dep_datetime,\n",
    "                                          timedelta_from_sched,\n",
    "                                          service_disruption,\n",
    "                                          cancellations\n",
    "                                     )\n",
    "                                     VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                     ON CONFLICT DO NOTHING; \n",
    "                                     \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_table_command = \"\"\"\n",
    "                               DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "                               CREATE TABLE weather_hourly (\n",
    "                                   location text,\n",
    "                                   date_time timestamp,\n",
    "                                   temperature real,\n",
    "                                   precipitation real,\n",
    "                                   cloud_cover real,\n",
    "                                   conditions text, \n",
    "                                   weather_type text,\n",
    "                                   latitude real,\n",
    "                                   longitude real,\n",
    "                                   PRIMARY KEY (date_time, location)\n",
    "                               );\n",
    "                               \"\"\"\n",
    "\n",
    "insert_into_weather_table_command = \"\"\"\n",
    "                                    INSERT INTO weather_hourly (\n",
    "                                        location,\n",
    "                                        date_time,\n",
    "                                        temperature,\n",
    "                                        precipitation,\n",
    "                                        cloud_cover,\n",
    "                                        conditions,\n",
    "                                        weather_type,\n",
    "                                        latitude,\n",
    "                                        longitude\n",
    "                                    )\n",
    "                                    VALUES\n",
    "                                        (%s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                                    ON CONFLICT DO NOTHING;\n",
    "                                    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f346bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_route_table_command = \"\"\"\n",
    "                             DROP TABLE IF EXISTS regional_route CASCADE;\n",
    "                            \n",
    "                             CREATE TABLE regional_route (\n",
    "                                 coord_id SERIAL PRIMARY KEY,\n",
    "                                 longitude real,\n",
    "                                 latitude real,\n",
    "                                 path_group numeric,\n",
    "                                 connecting_path text, \n",
    "                                 nb_station_group text,\n",
    "                                 sb_station_group text\n",
    "                             );\n",
    "                             \"\"\"\n",
    "\n",
    "insert_into_route_table_command = \"\"\"\n",
    "                                  INSERT INTO regional_route (\n",
    "                                      longitude,\n",
    "                                      latitude, \n",
    "                                      path_group,\n",
    "                                      connecting_path,\n",
    "                                      nb_station_group,\n",
    "                                      sb_station_group\n",
    "                                  )\n",
    "                                  VALUES \n",
    "                                      (%s, %s, %s, %s, %s, %s) \n",
    "                                  ON CONFLICT DO NOTHING;\n",
    "                                  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ac833",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_commands = [create_station_info_table_command,\n",
    "                         create_trains_table_command,\n",
    "                         create_weather_table_command,\n",
    "                         create_route_table_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='amtrakproject' user='{}' password={}\".format(os.environ.get('USER'), os.environ.get('DB_PASS')))\n",
    "assert conn is not None, 'need to fix conn!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "for command in create_table_commands:\n",
    "    create_table(conn, command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all station facts into station info table\n",
    "update_table(conn, insert_into_station_info_table_command, './data/facts/geo_stations_info.csv')\n",
    "\n",
    "# Insert route with the coordiniates into route table\n",
    "update_table(conn, insert_into_route_table_command, './data/facts/NE_regional_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdde067",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_trains_table_command)\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "begin_everything = time.time()\n",
    "\n",
    "# Insert all train data into arrival and departure data tables\n",
    "for year in years:\n",
    "    start = time.time()\n",
    "    arrive_csv = './data/trains/arrive_{}_processed.csv'.format(year)\n",
    "    depart_csv = './data/trains/depart_{}_processed.csv'.format(year)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Arrival', arrive_csv)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Departure', depart_csv)\n",
    "    print('DONE WITH', year, 'in', time.time() - start)\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1facf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(conn, create_weather_table_command)\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'Westerly_RI', 'Mystic_CT',\n",
    "                            'New_London_CT', 'Old_Saybrook_CT', 'New_Haven_CT', 'Bridgeport_CT', \n",
    "                            'Stamford_CT', 'New_Rochelle_NY', 'Manhattan_NY', 'Newark_NJ', 'Iselin_NJ', \n",
    "                            'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE','Aberdeen_MD', 'Baltimore_MD',\n",
    "                            'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all weather data into the weather data table\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        weather_csv = './data/weather/{}_weather_subset_{}.csv'.format(location, year)\n",
    "        update_table(conn, insert_into_weather_table_command, weather_csv)\n",
    "    print('Finished adding location', location, 'to the database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328db4e",
   "metadata": {},
   "source": [
    "# Part 3b: Loading Data into Postgres Database (Serial Primary Key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf687fe",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db3336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "import time\n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0935e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, command):\n",
    "    \"\"\"\n",
    "    Create a table in the PostgreSQL database from the specified command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "def update_table(conn, command, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from a CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "        \n",
    "def update_trains(conn, command, arr_or_dep, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from trains CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple([arr_or_dep] + row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930aaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_station_info_table = \"\"\" \n",
    "                            DROP TABLE IF EXISTS station_info CASCADE;\n",
    "\n",
    "                            CREATE TABLE station_info (\n",
    "                                station_id SERIAL PRIMARY KEY,\n",
    "                                station_code text,\n",
    "                                amtrak_station text,\n",
    "                                crew_change boolean,\n",
    "                                weather_station text,\n",
    "                                longitude real,\n",
    "                                latitude real,\n",
    "                                nb_mile numeric,\n",
    "                                sb_mile numeric\n",
    "                            );\n",
    "                            \"\"\"\n",
    "\n",
    "insert_into_station_info_table = \"\"\"\n",
    "                                 INSERT INTO station_info (\n",
    "                                     station_code,\n",
    "                                     amtrak_station,\n",
    "                                     crew_change,\n",
    "                                     weather_station,\n",
    "                                     longitude,\n",
    "                                     latitude,\n",
    "                                     nb_mile,\n",
    "                                     sb_mile\n",
    "                                 )\n",
    "                                 VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                 ON CONFLICT DO NOTHING;\n",
    "                                 \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02c6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trains_table = \"\"\" \n",
    "                      DROP TABLE IF EXISTS all_trains CASCADE;\n",
    "                      \n",
    "                      CREATE TABLE all_trains (\n",
    "                          dataset_id SERIAL PRIMARY KEY,\n",
    "                          arr_or_dep text,\n",
    "                          train_num text,\n",
    "                          station_code text, \n",
    "                          direction text,\n",
    "                          origin_date date,\n",
    "                          origin_year int,\n",
    "                          origin_month int,\n",
    "                          origin_week_day text,\n",
    "                          full_sched_arr_dep_datetime timestamp,\n",
    "                          sched_arr_dep_date date,\n",
    "                          sched_arr_dep_week_day text,\n",
    "                          sched_arr_dep_time time,\n",
    "                          act_arr_dep_time time,\n",
    "                          full_act_arr_dep_datetime timestamp,\n",
    "                          timedelta_from_sched numeric,\n",
    "                          service_disruption boolean,\n",
    "                          cancellations boolean\n",
    "                      );\n",
    "                      \"\"\"\n",
    "\n",
    "insert_into_trains_table = \"\"\"\n",
    "                           INSERT INTO all_trains (\n",
    "                               arr_or_dep,\n",
    "                               train_num,\n",
    "                               station_code,\n",
    "                               direction,\n",
    "                               origin_date,\n",
    "                               origin_year,\n",
    "                               origin_month,\n",
    "                               origin_week_day,\n",
    "                               full_sched_arr_dep_datetime,\n",
    "                               sched_arr_dep_date,\n",
    "                               sched_arr_dep_week_day,\n",
    "                               sched_arr_dep_time,\n",
    "                               act_arr_dep_time,\n",
    "                               full_act_arr_dep_datetime,\n",
    "                               timedelta_from_sched,\n",
    "                               service_disruption,\n",
    "                               cancellations\n",
    "                          )\n",
    "                          VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                          ON CONFLICT DO NOTHING; \n",
    "                          \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2eb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_table = \"\"\"\n",
    "                       DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "                       CREATE TABLE weather_hourly (\n",
    "                           weather_id SERIAL PRIMARY KEY,\n",
    "                           location text,\n",
    "                           date_time timestamp,\n",
    "                           temperature real,\n",
    "                           precipitation real,\n",
    "                           cloud_cover real,\n",
    "                           conditions text, \n",
    "                           weather_type text,\n",
    "                           latitude real,\n",
    "                           longitude real\n",
    "                       );\n",
    "                       \"\"\"\n",
    "\n",
    "insert_into_weather_table = \"\"\"\n",
    "                            INSERT INTO weather_hourly (\n",
    "                                location,\n",
    "                                date_time,\n",
    "                                temperature,\n",
    "                                precipitation,\n",
    "                                cloud_cover,\n",
    "                                conditions,\n",
    "                                weather_type,\n",
    "                                latitude,\n",
    "                                longitude\n",
    "                            )\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                            ON CONFLICT DO NOTHING;\n",
    "                            \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc97f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_route_table = \"\"\"\n",
    "                     DROP TABLE IF EXISTS regional_route CASCADE;\n",
    "\n",
    "                     CREATE TABLE regional_route (\n",
    "                         coord_id SERIAL PRIMARY KEY,\n",
    "                         longitude real,\n",
    "                         latitude real,\n",
    "                         path_group int,\n",
    "                         station_pairing text, \n",
    "                         nb_station_group text,\n",
    "                         sb_station_group text\n",
    "                     );\n",
    "                     \"\"\"\n",
    "\n",
    "insert_into_route_table = \"\"\"\n",
    "                          INSERT INTO\n",
    "                              regional_route (\n",
    "                                  longitude,\n",
    "                                  latitude, \n",
    "                                  path_group,\n",
    "                                  station_pairing,\n",
    "                                  nb_station_group,\n",
    "                                  sb_station_group\n",
    "                              )\n",
    "                          VALUES \n",
    "                              (%s, %s, %s, %s, %s, %s) \n",
    "                          ON CONFLICT DO NOTHING;\n",
    "                          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27ad026",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='amtrakproject' user='{}' password={}\".format(os.environ.get('USER'), os.environ.get('DB_PASS')))\n",
    "assert conn is not None, 'need to fix conn!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b09b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create station link table\n",
    "create_table(conn, create_station_info_table)\n",
    "\n",
    "# Create route coordinates table\n",
    "create_table(conn, create_route_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e087acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all station facts into station info table\n",
    "update_table(conn, insert_into_station_info_table, './data/facts/geo_stations_info.csv')\n",
    "\n",
    "# Insert route with the coordinates into route table\n",
    "update_table(conn, insert_into_route_table, './data/facts/NE_regional_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11be3822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding year 2011 to database in 4.946296215057373 seconds\n",
      "Finished adding year 2012 to database in 4.8167150020599365 seconds\n",
      "Finished adding year 2013 to database in 5.1157310009002686 seconds\n",
      "Finished adding year 2014 to database in 5.718309164047241 seconds\n",
      "Finished adding year 2015 to database in 5.917017221450806 seconds\n",
      "Finished adding year 2016 to database in 6.266315698623657 seconds\n",
      "Finished adding year 2017 to database in 6.2739222049713135 seconds\n",
      "Finished adding year 2018 to database in 6.24074912071228 seconds\n",
      "Finished adding year 2019 to database in 6.481274127960205 seconds\n",
      "Finished adding year 2020 to database in 4.550313949584961 seconds\n",
      "Finished adding year 2021 to database in 1.6248202323913574 seconds\n",
      "COMPLETE in 57.953227043151855\n"
     ]
    }
   ],
   "source": [
    "create_table(conn, create_trains_table)\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all train data into arrival and departure data tables\n",
    "begin_everything = time.time()\n",
    "for year in years:\n",
    "    start = time.time()\n",
    "    arrive_csv = './data/trains/arrive_{}_processed.csv'.format(year)\n",
    "    depart_csv = './data/trains/depart_{}_processed.csv'.format(year)\n",
    "    update_trains(conn, insert_into_trains_table, 'Arrival', arrive_csv)\n",
    "    update_trains(conn, insert_into_trains_table, 'Departure', depart_csv)\n",
    "    print('Finished adding year', year, 'to database in', time.time() - start, 'seconds')\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc85e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding location Boston_MA to database in 2.7271220684051514 seconds\n",
      "Finished adding location Providence_RI to database in 2.7188689708709717 seconds\n",
      "Finished adding location Kingston_RI to database in 2.6787750720977783 seconds\n",
      "Finished adding location Westerly_RI to database in 2.68355393409729 seconds\n",
      "Finished adding location Mystic_CT to database in 2.68345308303833 seconds\n",
      "Finished adding location New_London_CT to database in 2.692837953567505 seconds\n",
      "Finished adding location Old_Saybrook_CT to database in 2.6952991485595703 seconds\n",
      "Finished adding location New_Haven_CT to database in 2.7057251930236816 seconds\n",
      "Finished adding location Bridgeport_CT to database in 2.6888539791107178 seconds\n",
      "Finished adding location Stamford_CT to database in 2.6934330463409424 seconds\n",
      "Finished adding location New_Rochelle_NY to database in 2.7178261280059814 seconds\n",
      "Finished adding location Manhattan_NY to database in 2.7187106609344482 seconds\n",
      "Finished adding location Newark_NJ to database in 2.693528652191162 seconds\n",
      "Finished adding location Iselin_NJ to database in 2.7041428089141846 seconds\n",
      "Finished adding location Trenton_NJ to database in 2.706454038619995 seconds\n",
      "Finished adding location Philadelphia_PA to database in 2.7075231075286865 seconds\n",
      "Finished adding location Wilmington_DE to database in 2.6974098682403564 seconds\n",
      "Finished adding location Aberdeen_MD to database in 2.695342779159546 seconds\n",
      "Finished adding location Baltimore_MD to database in 2.712307929992676 seconds\n",
      "Finished adding location Baltimore_BWI_Airport_MD to database in 2.740086078643799 seconds\n",
      "Finished adding location New_Carrollton_MD to database in 2.7030251026153564 seconds\n",
      "Finished adding location Washington_DC to database in 2.8484668731689453 seconds\n",
      "COMPLETE in 59.615553855895996\n"
     ]
    }
   ],
   "source": [
    "create_table(conn, create_weather_table)\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'Westerly_RI', 'Mystic_CT',\n",
    "                            'New_London_CT', 'Old_Saybrook_CT', 'New_Haven_CT', 'Bridgeport_CT', \n",
    "                            'Stamford_CT', 'New_Rochelle_NY', 'Manhattan_NY', 'Newark_NJ', 'Iselin_NJ', \n",
    "                            'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE','Aberdeen_MD', 'Baltimore_MD',\n",
    "                            'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all weather data into the weather data table\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        weather_csv = './data/weather/{}_weather_subset_{}.csv'.format(location, year)\n",
    "        update_table(conn, insert_into_weather_table, weather_csv)\n",
    "    print('Finished adding location', location, 'to database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
