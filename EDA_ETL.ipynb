{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b64903",
   "metadata": {},
   "source": [
    "# ETL and EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bd0e4",
   "metadata": {},
   "source": [
    "# Part 1 - Amtrak Northeast Regional Train Data\n",
    "* This project would not be possible without the diligent joint effort by [Chris Juckins](https://juckins.net/index.php) and [John Bobinyec](http://dixielandsoftware.net/Amtrak/status/StatusMaps/) to collect and preserve Amtrak's on-time performance records. Chris Juckins' archive of timetables was another invaluable resource which enabled me to sort through the trains and stations I chose to use in this project.\n",
    "* The train data is sourced from [Amtrak Status Maps Archive Database (ASMAD)](https://juckins.net/amtrak_status/archive/html/home.php), and has been retrieved with Chris' permission.\n",
    "\n",
    "### Overview of the Process\n",
    "* Functions were written to scrape the HTML table returned from the search query and to process each column to the desired format\n",
    "* Additional columns were also added during processing to aid in joining the train data with weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770400b6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import re\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from trains_retrieve_and_process_data import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bee1b4",
   "metadata": {},
   "source": [
    "### Retrieve HTML table data and recreate as a Pandas DataFrame\n",
    "* Default is to collect data from the previous day (run after 5am or else no data will be retrieved, ASMAD updates around 4am)\n",
    "* Collects both arrival and departure data and stores in a dictionary further indexed by station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2021,4,21)\n",
    "end = date(2021,4,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = retrieve_data(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd909dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "depart =  raw_data_to_raw_df(raw_data, 'Depart')\n",
    "print(depart.shape[0])\n",
    "depart.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive = raw_data_to_raw_df(raw_data, 'Arrive')\n",
    "print(arrive.shape[0])\n",
    "arrive.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35503c7",
   "metadata": {},
   "source": [
    "### Save the raw DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring = './data/trains_raw/arrive_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "depart_filestring = './data/trains_raw/depart_raw_{}_{}.csv'.format(str(start), str(end))\n",
    "\n",
    "arrive.to_csv(arrive_filestring, line_terminator='\\n', index=False)\n",
    "depart.to_csv(depart_filestring, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd1957",
   "metadata": {},
   "source": [
    "### Process the raw DF with modifications/additions \n",
    "* Modifications to the data:\n",
    "    * Separate the Origin Date and Origin Week Day  into two columns\n",
    "    * Add separate columns for Origin Year and Origin Month\n",
    "    * Separate the Scheduled Arrival/Departure Date, Scheduled Arrival/Departure Week Day, and Scheduled Arrival/Departure Time into three seperate columns\n",
    "    * Calculate the value of the time difference between Scheduled and Actual Arrival/Departure\n",
    "    * Convert Service Disruption and Cancellation column text flags to binary indicator columns\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_arrive = process_columns(arrive, 'Arrive')\n",
    "full_arrive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depart = process_columns(depart, \"Depart\")\n",
    "full_depart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a9f09",
   "metadata": {},
   "source": [
    "### For new 2021 data, concatenate with previously retrieved and processed data from this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b52e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrive_filestring2021 = './data/trains/arrive_2021_processed.csv'\n",
    "depart_filestring2021 = './data/trains/depart_2021_processed.csv'\n",
    "        \n",
    "prev_arrive2021 = pd.read_csv(arrive_filestring2021)\n",
    "prev_depart2021 = pd.read_csv(depart_filestring2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021 = pd.concat([prev_arrive2021, full_arrive], ignore_index=True, axis=0)\n",
    "new_depart2021 = pd.concat([prev_depart2021, full_depart], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8436b",
   "metadata": {},
   "source": [
    "### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_arrive2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95aee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.drop_duplicates(inplace = True, ignore_index = True)\n",
    "new_depart2021.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a231e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_depart2021.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrive2021.to_csv(arrive_filestring2021, line_terminator='\\n', index=False)\n",
    "new_depart2021.to_csv(depart_filestring2021, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdecb55",
   "metadata": {},
   "source": [
    "# Part 2 - Visual Crossing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe05c71",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f27186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a70afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather_retrieve_and_process_data import *\n",
    "assert os.environ.get('VC_TOKEN') is not None , 'empty token!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068b6ac",
   "metadata": {},
   "source": [
    "### Retrieve unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = str(date.today()-timedelta(days=1))\n",
    "end = str(date.today()-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc081561",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_retrievals = retrieve_weather_data(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36c337",
   "metadata": {},
   "source": [
    "### Data Cleaning/Taking Subset of Columns\n",
    "\n",
    "* Processing recent data by year - add new columns, make minor fixes to string format, take subset of full columns list.\n",
    "* Function processes the files that were successfully created in the previous step.\n",
    "* This part is assuming 2021 data is being read and concatenates the previously retrieved data with the new data to create a single combined file.\n",
    "* Output shows the fraction of the data kept, data is valid and complete almost always ($> 99\\%$ of original data has been retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_weather_data(files_to_process=successful_retrievals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f95a2c",
   "metadata": {},
   "source": [
    "### Data sample for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/weather/Providence_RI_weather_2021_subset.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8693d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9be4bd",
   "metadata": {},
   "source": [
    "# Part 3: Loading Data into Postgres Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca8a15",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c5aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import os\n",
    "import sys \n",
    "import time\n",
    "assert os.environ.get('DB_PASS') != None , 'empty password!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8e2da",
   "metadata": {},
   "source": [
    "#### Functions to create and update tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73f6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, command):\n",
    "    \"\"\"\n",
    "    Create a table in the PostgreSQL database from the specified command.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "def update_table(conn, command, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from a CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple(row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() \n",
    "        \n",
    "def update_trains(conn, command, arr_or_dep, csv_file):\n",
    "    \"\"\"\n",
    "    Insert rows from trains CSV file into table specified by the command.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    with open(csv_file, newline='') as file:\n",
    "        info_reader = csv.reader(file, delimiter=',')\n",
    "        next(info_reader) # Skip header                                                                          \n",
    "        for row in info_reader:                                           \n",
    "            try:\n",
    "                cur.execute(command, tuple([arr_or_dep] + row))\n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(error)\n",
    "                conn.rollback()\n",
    "        conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99334dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_station_info_table_command = \"\"\" \n",
    "                                    DROP TABLE IF EXISTS station_info CASCADE;\n",
    "\n",
    "                                    CREATE TABLE station_info (\n",
    "                                        station_code text PRIMARY KEY,\n",
    "                                        station_name text,\n",
    "                                        state text,\n",
    "                                        amtrak_city text,\n",
    "                                        weather_loc text,\n",
    "                                        longitude real,\n",
    "                                        latitude real,\n",
    "                                        nb_mile numeric,\n",
    "                                        sb_mile numeric\n",
    "                                    );\n",
    "                                    \"\"\"\n",
    "\n",
    "insert_into_station_info_table_command = \"\"\"\n",
    "                                         INSERT INTO station_info (\n",
    "                                             station_code,\n",
    "                                             station_name,\n",
    "                                             state,\n",
    "                                             amtrak_city,\n",
    "                                             weather_loc,\n",
    "                                             longitude,\n",
    "                                             latitude,\n",
    "                                             nb_mile,\n",
    "                                             sb_mile\n",
    "                                        )\n",
    "                                        VALUES\n",
    "                                            (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                        ON CONFLICT DO NOTHING;\n",
    "                                        \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2a31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trains_table_command = \"\"\" \n",
    "                              DROP TABLE IF EXISTS all_trains CASCADE;\n",
    "                              CREATE TABLE all_trains (\n",
    "                                  arr_or_dep text,\n",
    "                                  train_num text,\n",
    "                                  station_code text REFERENCES station_info (station_code), \n",
    "                                  direction text,\n",
    "                                  origin_date date,\n",
    "                                  origin_year int,\n",
    "                                  origin_month int,\n",
    "                                  origin_week_day text,\n",
    "                                  full_sched_arr_dep_datetime timestamp,\n",
    "                                  sched_arr_dep_date date,\n",
    "                                  sched_arr_dep_week_day text,\n",
    "                                  sched_arr_dep_time time,\n",
    "                                  act_arr_dep_time time,\n",
    "                                  full_act_arr_dep_datetime timestamp,\n",
    "                                  timedelta_from_sched numeric,\n",
    "                                  service_disruption boolean,\n",
    "                                  cancellations boolean,\n",
    "                                  PRIMARY KEY (train_num, station_code, origin_date)\n",
    "                              );\n",
    "                              \"\"\"\n",
    "\n",
    "insert_into_trains_table_command = \"\"\"\n",
    "                                     INSERT INTO all_trains (\n",
    "                                          arr_or_dep,\n",
    "                                          train_num,\n",
    "                                          station_code,\n",
    "                                          direction,\n",
    "                                          origin_date,\n",
    "                                          origin_year,\n",
    "                                          origin_month,\n",
    "                                          origin_week_day,\n",
    "                                          full_sched_arr_dep_datetime,\n",
    "                                          sched_arr_dep_date,\n",
    "                                          sched_arr_dep_week_day,\n",
    "                                          sched_arr_dep_time,\n",
    "                                          act_arr_dep_time,\n",
    "                                          full_act_arr_dep_datetime,\n",
    "                                          timedelta_from_sched,\n",
    "                                          service_disruption,\n",
    "                                          cancellations\n",
    "                                     )\n",
    "                                     VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                     ON CONFLICT DO NOTHING; \n",
    "                                     \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b1b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_table_command = \"\"\"\n",
    "                               DROP TABLE IF EXISTS weather_hourly CASCADE;\n",
    "                               CREATE TABLE weather_hourly (\n",
    "                                   location text,\n",
    "                                   date_time timestamp,\n",
    "                                   temperature real,\n",
    "                                   precipitation real,\n",
    "                                   cloud_cover real,\n",
    "                                   conditions text, \n",
    "                                   weather_type text,\n",
    "                                   latitude real,\n",
    "                                   longitude real,\n",
    "                                   PRIMARY KEY (date_time, location)\n",
    "                               );\n",
    "                               \"\"\"\n",
    "\n",
    "insert_into_weather_table_command = \"\"\"\n",
    "                                    INSERT INTO weather_hourly (\n",
    "                                        location,\n",
    "                                        date_time,\n",
    "                                        temperature,\n",
    "                                        precipitation,\n",
    "                                        cloud_cover,\n",
    "                                        conditions,\n",
    "                                        weather_type,\n",
    "                                        latitude,\n",
    "                                        longitude\n",
    "                                    )\n",
    "                                    VALUES\n",
    "                                        (%s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                                    ON CONFLICT DO NOTHING;\n",
    "                                    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f346bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_route_table_command = \"\"\"\n",
    "                             DROP TABLE IF EXISTS regional_route CASCADE;\n",
    "                            \n",
    "                             CREATE TABLE regional_route (\n",
    "                                 coord_id SERIAL PRIMARY KEY,\n",
    "                                 longitude real,\n",
    "                                 latitude real,\n",
    "                                 path_group numeric,\n",
    "                                 connecting_path text, \n",
    "                                 nb_station_group text,\n",
    "                                 sb_station_group text\n",
    "                             );\n",
    "                             \"\"\"\n",
    "\n",
    "insert_into_route_table_command = \"\"\"\n",
    "                                  INSERT INTO regional_route (\n",
    "                                      longitude,\n",
    "                                      latitude, \n",
    "                                      path_group,\n",
    "                                      connecting_path,\n",
    "                                      nb_station_group,\n",
    "                                      sb_station_group\n",
    "                                  )\n",
    "                                  VALUES \n",
    "                                      (%s, %s, %s, %s, %s, %s) \n",
    "                                  ON CONFLICT DO NOTHING;\n",
    "                                  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25ac833",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_commands = [create_station_info_table_command,\n",
    "                         create_trains_table_command,\n",
    "                         create_weather_table_command,\n",
    "                         create_route_table_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7737b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='amtrakproject' user='{}' password={}\".format(os.environ.get('USER'), os.environ.get('DB_PASS')))\n",
    "assert conn is not None, 'need to fix conn!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "for command in create_table_commands:\n",
    "    create_table(conn, command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all station facts into station info table\n",
    "update_table(conn, insert_into_station_info_table_command, './data/facts/geo_stations_info.csv')\n",
    "\n",
    "# Insert route with the coordiniates into route table\n",
    "update_table(conn, insert_into_route_table_command, './data/facts/NE_regional_lonlat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecdde067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE WITH 2011 in 6.469912052154541\n",
      "DONE WITH 2012 in 6.21417498588562\n",
      "DONE WITH 2013 in 6.7026450634002686\n",
      "DONE WITH 2014 in 7.651795148849487\n",
      "DONE WITH 2015 in 7.8575828075408936\n",
      "DONE WITH 2016 in 8.140908002853394\n",
      "DONE WITH 2017 in 8.18492603302002\n",
      "DONE WITH 2018 in 8.250916719436646\n",
      "DONE WITH 2019 in 8.619727849960327\n",
      "DONE WITH 2020 in 6.061011075973511\n",
      "DONE WITH 2021 in 2.185502052307129\n",
      "COMPLETE in 76.34041380882263\n"
     ]
    }
   ],
   "source": [
    "create_table(conn, create_trains_table_command)\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "begin_everything = time.time()\n",
    "\n",
    "# Insert all train data into arrival and departure data tables\n",
    "for year in years:\n",
    "    start = time.time()\n",
    "    arrive_csv = './data/trains/arrive_{}_processed.csv'.format(year)\n",
    "    depart_csv = './data/trains/depart_{}_processed.csv'.format(year)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Arrival', arrive_csv)\n",
    "    update_trains(conn, insert_into_trains_table_command, 'Departure', depart_csv)\n",
    "    print('DONE WITH', year, 'in', time.time() - start)\n",
    "print('COMPLETE in', time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f1facf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding location Boston_MA to the database in 2.707117795944214 seconds\n",
      "Finished adding location Providence_RI to the database in 2.762277126312256 seconds\n",
      "Finished adding location Kingston_RI to the database in 2.7945449352264404 seconds\n",
      "Finished adding location Westerly_RI to the database in 2.8148820400238037 seconds\n",
      "Finished adding location Mystic_CT to the database in 2.8805429935455322 seconds\n",
      "Finished adding location New_London_CT to the database in 2.910943031311035 seconds\n",
      "Finished adding location Old_Saybrook_CT to the database in 2.9337141513824463 seconds\n",
      "Finished adding location New_Haven_CT to the database in 3.1269121170043945 seconds\n",
      "Finished adding location Bridgeport_CT to the database in 2.945215940475464 seconds\n",
      "Finished adding location Stamford_CT to the database in 2.9398610591888428 seconds\n",
      "Finished adding location New_Rochelle_NY to the database in 3.033431053161621 seconds\n",
      "Finished adding location Manhattan_NY to the database in 3.0119552612304688 seconds\n",
      "Finished adding location Newark_NJ to the database in 3.0035080909729004 seconds\n",
      "Finished adding location Iselin_NJ to the database in 2.956865072250366 seconds\n",
      "Finished adding location Trenton_NJ to the database in 2.9385039806365967 seconds\n",
      "Finished adding location Philadelphia_PA to the database in 3.2856619358062744 seconds\n",
      "Finished adding location Wilmington_DE to the database in 2.9313342571258545 seconds\n",
      "Finished adding location Aberdeen_MD to the database in 2.9132277965545654 seconds\n",
      "Finished adding location Baltimore_MD to the database in 3.049304723739624 seconds\n",
      "Finished adding location Baltimore_BWI_Airport_MD to the database in 3.0538699626922607 seconds\n",
      "Finished adding location New_Carrollton_MD to the database in 3.0606701374053955 seconds\n",
      "Finished adding location Washington_DC to the database in 3.026770830154419 seconds\n",
      "COMPLETE in 65.08361029624939\n"
     ]
    }
   ],
   "source": [
    "create_table(conn, create_weather_table_command)\n",
    "location_names_for_files = ['Boston_MA', 'Providence_RI', 'Kingston_RI', 'Westerly_RI', 'Mystic_CT',\n",
    "                            'New_London_CT', 'Old_Saybrook_CT', 'New_Haven_CT', 'Bridgeport_CT', \n",
    "                            'Stamford_CT', 'New_Rochelle_NY', 'Manhattan_NY', 'Newark_NJ', 'Iselin_NJ', \n",
    "                            'Trenton_NJ', 'Philadelphia_PA', 'Wilmington_DE','Aberdeen_MD', 'Baltimore_MD',\n",
    "                            'Baltimore_BWI_Airport_MD', 'New_Carrollton_MD', 'Washington_DC']\n",
    "\n",
    "years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# Insert all weather data into the weather data table\n",
    "begin_everything = time.time()\n",
    "for location in location_names_for_files:\n",
    "    start = time.time()\n",
    "    for year in years:\n",
    "        weather_csv = './data/weather/{}_weather_subset_{}.csv'.format(location, year)\n",
    "        update_table(conn, insert_into_weather_table_command, weather_csv)\n",
    "    print('Finished adding location', location, 'to the database in', time.time() - start, 'seconds')\n",
    "print(\"COMPLETE in\", time.time() - begin_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121e565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ee440",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_trip_identifier_table_command = \"\"\"\n",
    "                                       DROP IF EXISTS trips CASCADE;\n",
    "                                       CREATE TABLE trips \n",
    "                                           origin_date date,\n",
    "                                           train_num text\n",
    "                                       )\n",
    "                                       \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
